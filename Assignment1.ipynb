{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "random.seed(26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_corpus(corpus_filename):\n",
        "    with open(corpus_filename) as f:\n",
        "        corpus = f.read()\n",
        "        # find words\n",
        "        lowercased_corpus = corpus.lower()\n",
        "        all_words = re.findall(r'\\w+', lowercased_corpus)\n",
        "        for w in all_words:\n",
        "            if w == 'I':\n",
        "                print('Found')\n",
        "                break\n",
        "        unique_words = set(all_words)\n",
        "    return all_words, unique_words\n",
        "\n",
        "def get_words_frequencies(all_words):\n",
        "    word_freq_dict = {}\n",
        "    for word in all_words:\n",
        "        if word in word_freq_dict:\n",
        "            word_freq_dict[word] += 1\n",
        "        else:\n",
        "            word_freq_dict[word] = 1\n",
        "    return word_freq_dict\n",
        "\n",
        "def get_word_prob(word, all_words, word_freq_dict):\n",
        "    # check that the word exista in the vocabulary\n",
        "    if word in word_freq_dict:\n",
        "        return word_freq_dict[word] / len(all_words)\n",
        "    \n",
        "    return 0\n",
        "\n",
        "def add_char(word):\n",
        "    words_with_char_added = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            words_with_char_added.append(word[:i] + char + word[i:])\n",
        "        words_with_char_added.append(word + char)\n",
        "    return words_with_char_added\n",
        "\n",
        "def delete_char(word):\n",
        "    words_with_char_deleted = []\n",
        "    for i in range(len(word)):\n",
        "        words_with_char_deleted.append(word[:i] + word[i+1:])\n",
        "    return words_with_char_deleted\n",
        "\n",
        "def replace_char(word):\n",
        "    words_with_char_replaced = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            new_word = word[:i] + char + word[i+1:]\n",
        "            words_with_char_replaced.append(new_word)\n",
        "    return words_with_char_replaced\n",
        "\n",
        "def swap_chars(word):\n",
        "    words_with_chars_swapped = []\n",
        "    for i in range(len(word)-1):\n",
        "        new_word = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "        words_with_chars_swapped.append(new_word)\n",
        "    return words_with_chars_swapped\n",
        "\n",
        "def filter_existent_words(words, vocabulary):\n",
        "    return [word for word in words if word in vocabulary]\n",
        "\n",
        "def check_existence(word, vocabulary):\n",
        "    if word in vocabulary:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def generate_candidates_edit_1(word):\n",
        "    candidates = []\n",
        "    words_with_char_added = add_char(word)\n",
        "    #existent_words_with_char_added = filter_existent_words(words_with_char_added, vocabulary)\n",
        "    words_with_char_deleted = delete_char(word)\n",
        "    #existent_words_with_char_deleted = filter_existent_words(words_with_char_deleted, vocabulary)\n",
        "    words_with_char_replaced = replace_char(word)\n",
        "    #existent_words_with_char_replaced = filter_existent_words(words_with_char_replaced, vocabulary)\n",
        "    words_with_chars_swapped = swap_chars(word)\n",
        "\n",
        "    candidates.extend(words_with_char_added)\n",
        "    candidates.extend(words_with_char_deleted)\n",
        "    candidates.extend(words_with_char_replaced)\n",
        "    candidates.extend(words_with_chars_swapped)\n",
        "    unique_candidate_words = set(candidates)\n",
        "    if ('corrected' in unique_candidate_words):\n",
        "        print('Corrected found')\n",
        "    return unique_candidate_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of words in corpus: 1115585\n",
            "Num of unique words in corpus: 32198\n"
          ]
        }
      ],
      "source": [
        "all_words, unique_words = process_corpus('big.txt')\n",
        "print(f\"Num of words in corpus: {len(all_words)}\")\n",
        "print(f\"Num of unique words in corpus: {len(unique_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq = get_words_frequencies(all_words)\n",
        "word_freq['cat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_word_simple(word, vocabulary):\n",
        "    word = word.lower()\n",
        "    if check_existence(word, vocabulary):\n",
        "        return word\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(word)\n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "    if 'corrected' in candidates_edit_2:\n",
        "        print('Corrected found')\n",
        "    unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "\n",
        "    all_candidates = []\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, vocabulary)\n",
        "    unique_candidates_edit_2_existent = filter_existent_words(unique_candidates_edit_2, vocabulary)\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates.append((candidate, 1))\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates.append((candidate, 2))\n",
        "    unique_candidates = set(all_candidates)\n",
        "\n",
        "    # sort unique_candidates by the distance and the probability of the word\n",
        "    sorted_candidates = sorted(unique_candidates, key=lambda x: (x[1], -get_word_prob(x[0], all_words, word_freq)))\n",
        "    if len(sorted_candidates) > 0:\n",
        "        best_candidate = sorted_candidates[0]\n",
        "    else:\n",
        "        best_candidate = (word, 0)\n",
        "\n",
        "    return best_candidate[0]\n",
        "\n",
        "def correct_text(given_text):\n",
        "    found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "    cur_idx = 0\n",
        "    corrected_text = []\n",
        "    for cur_word_with_boundaries in found_words:\n",
        "        word = cur_word_with_boundaries.group()\n",
        "        start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "        corrected_word = correct_word_simple(word, unique_words)\n",
        "        # to save the spaces and punctuation\n",
        "        corrected_text.append(given_text[cur_idx:start_idx])\n",
        "        # if the word's characters are all UPPER\n",
        "        if word.isupper():\n",
        "            corrected_word = corrected_word.upper()\n",
        "        # if the first letter is in upper case\n",
        "        elif word.istitle():\n",
        "            corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "        corrected_text.append(corrected_word)\n",
        "        cur_idx = end_idx\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "    corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "    return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am a cat\n"
          ]
        }
      ],
      "source": [
        "corrected_text = correct_text(\"I am a cat7\")\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_word_simple('speling', unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is spelling correction task.\n"
          ]
        }
      ],
      "source": [
        "text = 'It is speling correction task.'\n",
        "corrected_text = correct_text(text)\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king sport'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_example = 'dking sport'\n",
        "correct_text(text_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_1_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word, freq = line.strip().split()\n",
        "            word = word.lower()\n",
        "            word_freq[word] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "def process_2_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word1, word2, freq = line.strip().split()\n",
        "            word1 = word1.lower()\n",
        "            word2 = word2.lower()\n",
        "            bigram = word1 + ' ' + word2\n",
        "            word_freq[bigram] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "# without Laplase smoothing (I commented it)\n",
        "# def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq):\n",
        "#     lowered_prev_word = prev_word.lower()\n",
        "#     lowered_cur_word = cur_word.lower()\n",
        "#     bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "#     total_single_word_freq = sum(single_word_freq.values())\n",
        "#     if bigram in bigram_freq:\n",
        "#         if lowered_prev_word in single_word_freq:\n",
        "#             return bigram_freq[bigram] / single_word_freq[lowered_prev_word]\n",
        "#         else:\n",
        "#             return bigram_freq[bigram] / total_single_word_freq\n",
        "#     else:\n",
        "#         if lowered_cur_word in single_word_freq:\n",
        "#             return single_word_freq[lowered_cur_word] / total_single_word_freq\n",
        "#         else:\n",
        "#             return 0\n",
        "        \n",
        "# adding Laplase smoothing\n",
        "def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq):\n",
        "    lowered_prev_word = prev_word.lower()\n",
        "    lowered_cur_word = cur_word.lower()\n",
        "    bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "    bigram_count = bigram_freq.get(bigram, 0)\n",
        "    prev_word_count = single_word_freq.get(lowered_prev_word, 0)\n",
        "    smoothed_prob = (bigram_count + 1)/ (prev_word_count + len(unique_words))\n",
        "    return smoothed_prob\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "def calculate_word_sequence_prob(words, bigram_freq, single_word_freq, prev_token = '<S>', edit_distance=1):\n",
        "    result = 0\n",
        "    for i in range(len(words)):\n",
        "        if i==0:\n",
        "            prob = calculate_bigram_prob(prev_token, words[i], bigram_freq, single_word_freq)\n",
        "        else:\n",
        "            prob = calculate_bigram_prob(words[i-1], words[i], bigram_freq, single_word_freq)\n",
        "        if prob == 0:\n",
        "            prob = 1e-10\n",
        "        result+= np.log(prob)\n",
        "        \n",
        "        # penalize for number of corrections\n",
        "        result = result - 0.05*edit_distance\n",
        "        # print(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_word_freq = process_1_word_freq('count_1w.txt')\n",
        "bigram_freq = process_2_word_freq('count_2w.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.259822215839525e-05\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('the', 'cat', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5741336593573144e-09\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('t', 'cot', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.6398877749638754\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['the', 'cat'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-12.593434927385083\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['t', 'cot'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def correct_word_bigram(given_word, given_text, given_word_idx):\n",
        "#     print(given_text)\n",
        "#     given_word = given_word.lower()\n",
        "#     if check_existence(given_word, unique_words):\n",
        "#         print('Given word is in the vocabulary')\n",
        "#         return given_word\n",
        "#     all_candidates_with_edit_dist = []\n",
        "#     unique_candidates_edit_1 = generate_candidates_edit_1(given_word)\n",
        "#     print('unique_candidates_edit_1', unique_candidates_edit_1)\n",
        "    \n",
        "#     candidates_edit_2 = []\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "#         candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "#     unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "#     print('unique_candidates_edit_2', len(unique_candidates_edit_2))\n",
        "\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 1))\n",
        "#     for candidate in unique_candidates_edit_2:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 2))\n",
        "\n",
        "#     all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "#     all_unique_candidates_with_edit_dist_existent = filter_existent_words(all_unique_candidates_with_edit_dist, unique_words)\n",
        "#     print('all_unique_candidates_with_edit_dist_existent', all_unique_candidates_with_edit_dist_existent)\n",
        "    \n",
        "#     # find best candidate\n",
        "#     new_probabilities = []\n",
        "#     for (candidate, edit_dist) in all_unique_candidates_with_edit_dist_existent:\n",
        "#         new_word_sequence = list(given_text.copy())\n",
        "#         new_word_sequence[given_word_idx] = candidate\n",
        "#         print('new_word_sequence', new_word_sequence)\n",
        "#         prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "#         new_probabilities.append(prob)\n",
        "#     if len(all_unique_candidates_with_edit_dist_existent) > 0:\n",
        "#         best_candidate = list(all_unique_candidates_with_edit_dist_existent)[new_probabilities.index(max(new_probabilities))]\n",
        "#     else:\n",
        "#         best_candidate = (given_word, 0)\n",
        "#     return best_candidate[0][0]\n",
        "\n",
        "# def correct_text_bigram(given_text):\n",
        "#     found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "#     cur_idx = 0\n",
        "#     corrected_text = []\n",
        "#     cur_word_idx = 0\n",
        "#     for cur_word_with_boundaries in found_words:\n",
        "#         cur_word_idx += 1\n",
        "#         word = cur_word_with_boundaries.group()\n",
        "#         start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "#         corrected_word = correct_word_bigram(word, given_text, cur_idx)\n",
        "#         # to save the spaces and punctuation\n",
        "#         corrected_text.append(given_text[cur_idx:start_idx])\n",
        "#         # if the word's characters are all UPPER\n",
        "#         if word.isupper():\n",
        "#             corrected_word = corrected_word.upper()\n",
        "#         # if the first letter is in upper case\n",
        "#         elif word.istitle():\n",
        "#             corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "#         corrected_text.append(corrected_word)\n",
        "#         cur_idx = end_idx\n",
        "#     corrected_text.append(given_text[cur_idx:])\n",
        "#     corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "#     return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def correct_word_bigram(given_word, given_text_tokens, given_word_idx):\n",
        "    print(\"Processing word:\", given_word)\n",
        "    given_word_lower = given_word.lower()\n",
        "    \n",
        "    # If the word is already correct, return it\n",
        "    if check_existence(given_word_lower, unique_words):\n",
        "        print('Given word is in the vocabulary')\n",
        "        return given_word\n",
        "\n",
        "    # Generate candidate corrections\n",
        "    all_candidates_with_edit_dist = []\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(given_word_lower)\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, unique_words)\n",
        "\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 1))\n",
        "    \n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_candidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_candidates_edit_2)\n",
        "    \n",
        "    unique_candidates_edit_2_existent = filter_existent_words(set(candidates_edit_2), unique_words)\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 2))\n",
        "    \n",
        "    all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "\n",
        "    if not all_unique_candidates_with_edit_dist:\n",
        "        return given_word \n",
        "    # Find the best correction based on probability\n",
        "    new_probabilities = []\n",
        "    for (candidate, edit_dist) in all_unique_candidates_with_edit_dist:\n",
        "        new_word_sequence = given_text_tokens.copy()\n",
        "        new_word_sequence[given_word_idx] = candidate\n",
        "        prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "        new_probabilities.append(prob)\n",
        "    best_candidate = list(all_unique_candidates_with_edit_dist)[new_probabilities.index(max(new_probabilities))][0]\n",
        "\n",
        "\n",
        "    # Preserve capitalization\n",
        "    if given_word.isupper():\n",
        "        return best_candidate.upper()\n",
        "    elif given_word.istitle():\n",
        "        return best_candidate.capitalize()\n",
        "    else:\n",
        "        return best_candidate\n",
        "\n",
        "def correct_text_bigram(given_text):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', given_text))\n",
        "    corrected_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrected_text.append(given_text[cur_idx:start])\n",
        "\n",
        "        corrected_word = correct_word_bigram(word, [m.group() for m in found_words], idx)\n",
        "        \n",
        "        # Append corrected word\n",
        "        corrected_text.append(corrected_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I am a cat. Hello!'"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('I am a cat7. Hello!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king sport'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('dking sport')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king species'"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('dking species')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates for 'dking': {'dkinyg', 'dving', 'doing', 'uking', 'dkaing', 'dkwing', 'dkidg', 'sking', 'daking', 'dkiny', 'dtking', 'dkinvg', 'dkifg', 'dkinz', 'dkitng', 'dkong', 'dkingg', 'hking', 'dkixng', 'djking', 'xking', 'dkivng', 'dkilng', 'dning', 'dkiung', 'dkinx', 'dling', 'dying', 'dkinc', 'dkiyng', 'dkeing', 'dkixg', 'dkijng', 'dkicg', 'bdking', 'dkinl', 'dkxing', 'dfing', 'dkibng', 'dkieg', 'dkinw', 'gdking', 'dkiqng', 'bking', 'dkipg', 'dkiang', 'dkcng', 'kking', 'mdking', 'dcking', 'dikng', 'diing', 'dkinig', 'fking', 'dkpng', 'dqking', 'wking', 'zking', 'dkina', 'dkind', 'dcing', 'dvking', 'dkinpg', 'edking', 'ldking', 'yking', 'dkinsg', 'dkigng', 'dkine', 'dkning', 'dfking', 'dgking', 'dkinjg', 'dkang', 'dkisng', 'dkinag', 'dkiag', 'dkyng', 'dwing', 'dging', 'dkfng', 'fdking', 'dkindg', 'rdking', 'dkinb', 'cdking', 'dkizg', 'dknng', 'idking', 'dkink', 'dkding', 'dkilg', 'xdking', 'dkting', 'dxking', 'dkqing', 'dkino', 'dkping', 'duking', 'dsing', 'dbing', 'dkinkg', 'dkrng', 'hdking', 'dkineg', 'dkiog', 'dkibg', 'dkinlg', 'dkidng', 'pking', 'dkming', 'dkinmg', 'dkwng', 'dkikg', 'dkinn', 'ddking', 'dkikng', 'dkinug', 'dkeng', 'dkiqg', 'dkiwng', 'odking', 'dkvng', 'jdking', 'nking', 'dbking', 'qking', 'dkhng', 'dming', 'dmking', 'dkzng', 'dkbing', 'dkiing', 'dkung', 'dknig', 'dkoing', 'dkizng', 'dkimg', 'dkjing', 'deking', 'dping', 'dkgng', 'adking', 'wdking', 'dkin', 'zdking', 'dkinog', 'duing', 'dting', 'deing', 'dkkng', 'dkinbg', 'qdking', 'rking', 'dkbng', 'dkinf', 'dkinng', 'dkking', 'dkmng', 'gking', 'drking', 'dkving', 'aking', 'dzing', 'dpking', 'daing', 'dkqng', 'pdking', 'ndking', 'udking', 'dwking', 'dkinwg', 'dkinfg', 'eking', 'vking', 'dqing', 'dxing', 'dking', 'dkinr', 'dkling', 'dkincg', 'dkhing', 'dkxng', 'dring', 'kdking', 'dkying', 'dhing', 'dkinh', 'dkifng', 'dkimng', 'dkint', 'dkieng', 'dkiyg', 'doking', 'dkinrg', 'dkinxg', 'dklng', 'dkijg', 'sdking', 'dkivg', 'dkiig', 'dksng', 'vdking', 'mking', 'diking', 'dkihng', 'dkini', 'dsking', 'dding', 'dkicng', 'ydking', 'dhking', 'dkig', 'dkirg', 'dkdng', 'dkinp', 'kding', 'dzking', 'dkng', 'dlking', 'dyking', 'dkintg', 'dnking', 'dkigg', 'dkcing', 'dkiong', 'dkiug', 'dkins', 'dkjng', 'dksing', 'dkging', 'iking', 'dkring', 'dkinu', 'dktng', 'dkinv', 'dkingm', 'oking', 'dkinq', 'dkihg', 'lking', 'ding', 'dkzing', 'dkfing', 'dkinm', 'dkinhg', 'dkinqg', 'dkign', 'dkipng', 'cking', 'tking', 'djing', 'dkirng', 'dkisg', 'dkuing', 'dkiwg', 'dkitg', 'tdking', 'jking', 'dkinj', 'dkinzg', 'king'}\n"
          ]
        }
      ],
      "source": [
        "word='dking'\n",
        "candidates = generate_candidates_edit_1(word)\n",
        "print(f\"Candidates for '{word}': {candidates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram count: 0\n",
            "Bigram count: 0\n",
            "Single word count: 9123557\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram count:\", bigram_freq.get(\"dying sport\", 0))\n",
        "print(\"Bigram count:\", bigram_freq.get(\"dying species\", 0))\n",
        "print(\"Single word count:\", single_word_freq.get(\"dying\", 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying trigrams\n",
        "Dataset with trigrams\n",
        "https://calmcode.io/datasets/english_3grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difficulties\n",
        "\n",
        "multiplication of probabilities fastly becomes 0 => use sum of logarithms\n",
        "for unseen words the probabilities are similar as for 'dying species' and 'dyong sport'\n",
        "\n",
        "#### Difficulties\n",
        "**Capturing context**\n",
        "\n",
        "- moving from unigrams to bigrams\n",
        "- moving from **bigrams** to **trigrams**\n",
        "\n",
        "With bigrams for phrases of 2 words the context is not captured. In the given example, for the word `dking` we just see the start token `<S>` and do not see the next word: `sport` or `species`. Therefore, I decided to use trigrams.\n",
        "\n",
        "- no trigram \n",
        "### Ideas\n",
        "- backoff\n",
        "- keyboard layout\n",
        "- dataset larger\n",
        "- forward and backward\n",
        "- несколькр слов подряд некорректных - заменять на скорректированное\n",
        "- использовать стеммы?\n",
        "- добавить swap\n",
        "- стемминг\n",
        "\n",
        "\n",
        "Вместо big можно вот этот попробовать https://www.kaggle.com/datasets/ironicninja/coca-dataset?select=COCA_tokens.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing my unigram model with Norwigs\n",
        "\n",
        "1. I noticed the difference in the training corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32198"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115585"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79809"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('quintessential', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07154004401278254"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('the', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'unit_tests pass'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "# Norvig tests\n",
        "def unit_tests():\n",
        "    assert correct_text('speling') == 'spelling'              # insert\n",
        "    assert correct_text('korrectud') == 'corrected'           # replace 2\n",
        "    assert correct_text('bycycle') == 'bicycle'               # replace\n",
        "    assert correct_text('inconvient') == 'inconvenient'       # insert 2\n",
        "    assert correct_text('arrainged') == 'arranged'            # delete\n",
        "    assert correct_text('peotry') =='poetry'                  # transpose\n",
        "    assert correct_text('peotryy') =='poetry'                 # transpose + delete\n",
        "    assert correct_text('word') == 'word'                     # known\n",
        "    assert correct_text('quintessential') == 'quintessential' # unknown\n",
        "    # assert process_corpus('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
        "    # assert len(unique_words) == 32192\n",
        "    # assert len(all_words) == 1115504\n",
        "    # assert all_words.most_common(10) == [\n",
        "    #  ('the', 79808),\n",
        "    #  ('of', 40024),\n",
        "    #  ('and', 38311),\n",
        "    #  ('to', 28765),\n",
        "    #  ('in', 22020),\n",
        "    #  ('a', 21124),\n",
        "    #  ('that', 12512),\n",
        "    #  ('he', 12401),\n",
        "    #  ('was', 11410),\n",
        "    #  ('it', 10681)]\n",
        "    # assert all_words['the'] == 79808\n",
        "    assert get_word_prob('quintessential', all_words, word_freq) == 0\n",
        "    assert 0.07 < get_word_prob('the', all_words, word_freq) < 0.08\n",
        "    return 'unit_tests pass'\n",
        "\n",
        "unit_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "correction(contende) => contend (3); expected contented (13)\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "correction(guic) => guns (111); expected juice (5)\n",
            "correction(juce) => june (44); expected juice (5)\n",
            "correction(jucie) => julie (71); expected juice (5)\n",
            "correction(juise) => guise (8); expected juice (5)\n",
            "correction(juse) => just (767); expected juice (5)\n",
            "correction(localy) => local (181); expected locally (10)\n",
            "correction(compair) => company (190); expected compare (29)\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "correction(futher) => father (533); expected further (138)\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "correction(receit) => recent (53); expected receipt (13)\n",
            "correction(receite) => receive (95); expected receipt (13)\n",
            "correction(reciet) => recite (4); expected receipt (13)\n",
            "correction(remined) => remained (231); expected remind (9)\n",
            "correction(annt) => anna (294); expected aunt (52)\n",
            "correction(ther) => the (79809); expected there (2972)\n",
            "correction(totaly) => total (35); expected totally (9)\n",
            "correction(vistid) => viscid (3); expected visited (28)\n",
            "correction(ment) => men (1145); expected meant (113)\n",
            "correction(sorces) => forces (176); expected sources (30)\n",
            "correction(desicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "correction(splened) => opened (216); expected splendid (77)\n",
            "correction(acount) => count (748); expected account (177)\n",
            "correction(semetary) => secretary (52); expected cemetery (2)\n",
            "correction(lates) => later (334); expected latest (17)\n",
            "correction(rember) => member (50); expected remember (161)\n",
            "correction(cak) => can (1095); expected cake (6)\n",
            "correction(chosing) => closing (35); expected choosing (20)\n",
            "correction(rote) => rose (243); expected wrote (149)\n",
            "correction(awfall) => wall (189); expected awful (29)\n",
            "correction(lauf) => last (565); expected laugh (70)\n",
            "correction(laught) => caught (90); expected laugh (70)\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "correction(pomes) => comes (91); expected poems (3)\n",
            "correction(perple) => people (891); expected purple (29)\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "correction(muinets) => muskets (22); expected minutes (146)\n",
            "correction(aranging) => arranging (19); expected arrangeing (0)\n",
            "correction(accesing) => acceding (1); expected accessing (0)\n",
            "correction(stomec) => some (1536); expected stomach (42)\n",
            "correction(embaras) => embargo (7); expected embarrass (0)\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "correction(failes) => failed (63); expected fails (20)\n",
            "correction(poame) => some (1536); expected poem (6)\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "correction(lones) => bones (257); expected loans (13)\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "correction(choise) => choose (54); expected choice (46)\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "correction(cartains) => captains (12); expected curtains (5)\n",
            "correction(certans) => certains (1); expected curtains (5)\n",
            "correction(courtens) => countess (497); expected curtains (5)\n",
            "correction(curtions) => portions (56); expected curtains (5)\n",
            "correction(adress) => dress (138); expected address (76)\n",
            "correction(adres) => acres (36); expected address (76)\n",
            "correction(superceed) => superseded (9); expected supersede (1)\n",
            "74% of 270 correct (6% unknown) at 21 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_word_simple(wrong, unique_words)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "Processing word: contenpted\n",
            "Processing word: contende\n",
            "correction(contende) => content (29); expected contented (13)\n",
            "Processing word: contended\n",
            "Given word is in the vocabulary\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "Processing word: contentid\n",
            "correction(contentid) => content (29); expected contented (13)\n",
            "Processing word: begining\n",
            "Processing word: problam\n",
            "correction(problam) => program (43); expected problem (71)\n",
            "Processing word: proble\n",
            "correction(proble) => people (891); expected problem (71)\n",
            "Processing word: promblem\n",
            "Processing word: proplen\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "Processing word: dirven\n",
            "correction(dirven) => given (364); expected driven (66)\n",
            "Processing word: exstacy\n",
            "correction(exstacy) => eustace (1); expected ecstasy (8)\n",
            "Processing word: ecstacy\n",
            "Processing word: guic\n",
            "correction(guic) => music (56); expected juice (5)\n",
            "Processing word: juce\n",
            "correction(juce) => such (1436); expected juice (5)\n",
            "Processing word: jucie\n",
            "correction(jucie) => judge (45); expected juice (5)\n",
            "Processing word: juise\n",
            "correction(juise) => use (320); expected juice (5)\n",
            "Processing word: juse\n",
            "correction(juse) => else (201); expected juice (5)\n",
            "Processing word: localy\n",
            "correction(localy) => local (181); expected locally (10)\n",
            "Processing word: compair\n",
            "Processing word: pronounciation\n",
            "Processing word: transportibility\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "Processing word: miniscule\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "Processing word: independant\n",
            "Processing word: independant\n",
            "Processing word: aranged\n",
            "correction(aranged) => range (39); expected arranged (75)\n",
            "Processing word: arrainged\n",
            "Processing word: poartry\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "Processing word: poertry\n",
            "correction(poertry) => poverty (26); expected poetry (10)\n",
            "Processing word: poetre\n",
            "Processing word: poety\n",
            "correction(poety) => post (114); expected poetry (10)\n",
            "Processing word: powetry\n",
            "correction(powetry) => power (545); expected poetry (10)\n",
            "Processing word: leval\n",
            "correction(leval) => local (181); expected level (53)\n",
            "Processing word: basicaly\n",
            "Processing word: triangulaur\n",
            "Processing word: unexpcted\n",
            "Processing word: unexpeted\n",
            "Processing word: unexspected\n",
            "Processing word: stanerdizing\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "Processing word: varable\n",
            "Processing word: futher\n",
            "correction(futher) => other (1501); expected further (138)\n",
            "Processing word: monitering\n",
            "Processing word: biscits\n",
            "correction(biscits) => visits (11); expected biscuits (8)\n",
            "Processing word: biscutes\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "Processing word: biscuts\n",
            "correction(biscuts) => discuss (36); expected biscuits (8)\n",
            "Processing word: bisquits\n",
            "Processing word: buiscits\n",
            "Processing word: buiscuts\n",
            "Processing word: avaible\n",
            "Processing word: seperate\n",
            "Processing word: neccesary\n",
            "Processing word: necesary\n",
            "Processing word: neccesary\n",
            "Processing word: necassary\n",
            "Processing word: necassery\n",
            "Processing word: neccasary\n",
            "Processing word: defenition\n",
            "Processing word: receit\n",
            "correction(receit) => recent (53); expected receipt (13)\n",
            "Processing word: receite\n",
            "correction(receite) => recent (53); expected receipt (13)\n",
            "Processing word: reciet\n",
            "correction(reciet) => recent (53); expected receipt (13)\n",
            "Processing word: recipt\n",
            "correction(recipt) => recent (53); expected receipt (13)\n",
            "Processing word: remine\n",
            "correction(remine) => remove (53); expected remind (9)\n",
            "Processing word: remined\n",
            "correction(remined) => defined (55); expected remind (9)\n",
            "Processing word: inetials\n",
            "correction(inetials) => initial (18); expected initials (7)\n",
            "Processing word: inistals\n",
            "correction(inistals) => install (1); expected initials (7)\n",
            "Processing word: initails\n",
            "correction(initails) => initial (18); expected initials (7)\n",
            "Processing word: initals\n",
            "correction(initals) => initial (18); expected initials (7)\n",
            "Processing word: intials\n",
            "correction(intials) => initial (18); expected initials (7)\n",
            "Processing word: magnificnet\n",
            "Processing word: magificent\n",
            "Processing word: magnifcent\n",
            "Processing word: magnifecent\n",
            "Processing word: magnifiscant\n",
            "Processing word: magnifisent\n",
            "Processing word: magnificant\n",
            "correction(magnificant) => significant (39); expected magnificent (12)\n",
            "Processing word: annt\n",
            "correction(annt) => and (38312); expected aunt (52)\n",
            "Processing word: anut\n",
            "correction(anut) => and (38312); expected aunt (52)\n",
            "Processing word: arnt\n",
            "correction(arnt) => and (38312); expected aunt (52)\n",
            "Processing word: intial\n",
            "correction(intial) => until (325); expected initial (18)\n",
            "Processing word: ther\n",
            "correction(ther) => the (79809); expected there (2972)\n",
            "Processing word: experances\n",
            "Processing word: biult\n",
            "correction(biult) => but (5653); expected built (77)\n",
            "Processing word: totaly\n",
            "correction(totaly) => total (35); expected totally (9)\n",
            "Processing word: undersand\n",
            "Processing word: undistand\n",
            "Processing word: southen\n",
            "correction(southen) => south (311); expected southern (194)\n",
            "Processing word: definately\n",
            "Processing word: difinately\n",
            "Processing word: fisited\n",
            "correction(fisited) => limited (79); expected visited (28)\n",
            "Processing word: viseted\n",
            "correction(viseted) => listed (3); expected visited (28)\n",
            "Processing word: vistid\n",
            "correction(vistid) => visit (81); expected visited (28)\n",
            "Processing word: vistied\n",
            "correction(vistied) => listed (3); expected visited (28)\n",
            "Processing word: volantry\n",
            "Processing word: ment\n",
            "correction(ment) => next (277); expected meant (113)\n",
            "Processing word: recieve\n",
            "Processing word: sorces\n",
            "correction(sorces) => source (94); expected sources (30)\n",
            "Processing word: wether\n",
            "correction(wether) => other (1501); expected whether (357)\n",
            "Processing word: usefull\n",
            "Processing word: litriture\n",
            "Processing word: valubale\n",
            "Processing word: valuble\n",
            "correction(valuble) => value (106); expected valuable (33)\n",
            "Processing word: desicate\n",
            "correction(desicate) => designate (1); expected desiccate (0)\n",
            "Processing word: dessicate\n",
            "correction(dessicate) => dedicate (1); expected desiccate (0)\n",
            "Processing word: dessiccate\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "Processing word: clearical\n",
            "Processing word: spledid\n",
            "Processing word: splended\n",
            "Processing word: splened\n",
            "correction(splened) => speed (31); expected splendid (77)\n",
            "Processing word: splended\n",
            "Processing word: beetween\n",
            "Processing word: completly\n",
            "correction(completly) => complete (144); expected completely (94)\n",
            "Processing word: acount\n",
            "correction(acount) => about (1497); expected account (177)\n",
            "Processing word: cemetary\n",
            "Processing word: semetary\n",
            "Processing word: speaical\n",
            "Processing word: specail\n",
            "Processing word: specal\n",
            "Processing word: speical\n",
            "Processing word: lates\n",
            "correction(lates) => last (565); expected latest (17)\n",
            "Processing word: latets\n",
            "correction(latets) => rates (47); expected latest (17)\n",
            "Processing word: latiest\n",
            "Processing word: latist\n",
            "correction(latist) => last (565); expected latest (17)\n",
            "Processing word: perhapse\n",
            "Processing word: rember\n",
            "correction(rember) => number (301); expected remember (161)\n",
            "Processing word: remeber\n",
            "correction(remeber) => member (50); expected remember (161)\n",
            "Processing word: rememmer\n",
            "Processing word: rermember\n",
            "Processing word: chaper\n",
            "correction(chaper) => paper (177); expected chapter (464)\n",
            "Processing word: chaphter\n",
            "Processing word: chaptur\n",
            "Processing word: cak\n",
            "correction(cak) => a (21124); expected cake (6)\n",
            "Processing word: vairious\n",
            "Processing word: febuary\n",
            "Processing word: pertend\n",
            "correction(pertend) => percent (1); expected pretend (8)\n",
            "Processing word: protend\n",
            "correction(protend) => protein (4); expected pretend (8)\n",
            "Processing word: prtend\n",
            "correction(prtend) => friend (283); expected pretend (8)\n",
            "Processing word: pritend\n",
            "correction(pritend) => printed (27); expected pretend (8)\n",
            "Processing word: chosing\n",
            "correction(chosing) => housing (3); expected choosing (20)\n",
            "Processing word: rote\n",
            "correction(rote) => more (1997); expected wrote (149)\n",
            "Processing word: wote\n",
            "correction(wote) => more (1997); expected wrote (149)\n",
            "Processing word: particulaur\n",
            "Processing word: awfall\n",
            "correction(awfall) => fall (124); expected awful (29)\n",
            "Processing word: afful\n",
            "Processing word: arragment\n",
            "Processing word: chalenges\n",
            "correction(chalenges) => changes (163); expected challenges (2)\n",
            "Processing word: chalenges\n",
            "correction(chalenges) => changes (163); expected challenges (2)\n",
            "Processing word: lagh\n",
            "correction(lagh) => page (59); expected laugh (70)\n",
            "Processing word: lauf\n",
            "correction(lauf) => last (565); expected laugh (70)\n",
            "Processing word: laught\n",
            "correction(laught) => light (277); expected laugh (70)\n",
            "Processing word: lugh\n",
            "correction(lugh) => such (1436); expected laugh (70)\n",
            "Processing word: ofen\n",
            "correction(ofen) => of (40024); expected often (443)\n",
            "Processing word: offen\n",
            "correction(offen) => open (322); expected often (443)\n",
            "Processing word: offten\n",
            "Processing word: ofton\n",
            "Processing word: somone\n",
            "correction(somone) => some (1536); expected someone (160)\n",
            "Processing word: personnell\n",
            "Processing word: uneque\n",
            "Processing word: diagrammaticaally\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "Processing word: discription\n",
            "Processing word: poims\n",
            "correction(poims) => point (223); expected poems (3)\n",
            "Processing word: pomes\n",
            "correction(pomes) => home (294); expected poems (3)\n",
            "Processing word: perple\n",
            "correction(perple) => people (891); expected purple (29)\n",
            "Processing word: perpul\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "Processing word: poarple\n",
            "Processing word: descide\n",
            "correction(descide) => describe (51); expected decide (33)\n",
            "Processing word: articals\n",
            "Processing word: possition\n",
            "Processing word: extented\n",
            "correction(extented) => expected (126); expected extended (75)\n",
            "Processing word: hierachial\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "Processing word: realy\n",
            "correction(realy) => read (219); expected really (272)\n",
            "Processing word: relley\n",
            "correction(relley) => reply (166); expected really (272)\n",
            "Processing word: relly\n",
            "correction(relly) => well (1198); expected really (272)\n",
            "Processing word: voteing\n",
            "correction(voteing) => nothing (646); expected voting (6)\n",
            "Processing word: comittee\n",
            "Processing word: wantid\n",
            "correction(wantid) => want (323); expected wanted (213)\n",
            "Processing word: wonted\n",
            "Given word is in the vocabulary\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "Processing word: benifits\n",
            "Processing word: defenitions\n",
            "correction(defenitions) => definition (23); expected definitions (3)\n",
            "Processing word: scisors\n",
            "Processing word: sissors\n",
            "correction(sissors) => sisters (16); expected scissors (19)\n",
            "Processing word: levals\n",
            "correction(levals) => level (53); expected levels (1)\n",
            "Processing word: paralel\n",
            "Processing word: paralell\n",
            "Processing word: parrallel\n",
            "Processing word: parralell\n",
            "Processing word: parrallell\n",
            "Processing word: accomodation\n",
            "Processing word: acommodation\n",
            "Processing word: acomodation\n",
            "Processing word: planed\n",
            "Given word is in the vocabulary\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "Processing word: hierchy\n",
            "Processing word: transfred\n",
            "correction(transfred) => transfer (20); expected transferred (52)\n",
            "Processing word: muinets\n",
            "correction(muinets) => mines (22); expected minutes (146)\n",
            "Processing word: aranging\n",
            "correction(aranging) => changing (43); expected arrangeing (0)\n",
            "Processing word: accesing\n",
            "correction(accesing) => accepting (12); expected accessing (0)\n",
            "Processing word: stomac\n",
            "correction(stomac) => atomic (1); expected stomach (42)\n",
            "Processing word: stomache\n",
            "Processing word: stomec\n",
            "correction(stomec) => some (1536); expected stomach (42)\n",
            "Processing word: stumache\n",
            "Processing word: unfortunatly\n",
            "Processing word: conciderable\n",
            "Processing word: acess\n",
            "Processing word: singulaur\n",
            "Processing word: scarcly\n",
            "Processing word: scarecly\n",
            "Processing word: scarely\n",
            "correction(scarely) => surely (24); expected scarcely (65)\n",
            "Processing word: scarsely\n",
            "Processing word: questionaire\n",
            "Processing word: experance\n",
            "Processing word: experiance\n",
            "Processing word: possable\n",
            "Processing word: reafreshment\n",
            "Processing word: refreshmant\n",
            "Processing word: refresment\n",
            "correction(refresment) => represent (16); expected refreshment (4)\n",
            "Processing word: refressmunt\n",
            "Processing word: embaras\n",
            "correction(embaras) => embers (4); expected embarrass (0)\n",
            "Processing word: embarass\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "Processing word: vistors\n",
            "correction(vistors) => history (348); expected visitors (69)\n",
            "Processing word: auxillary\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "Processing word: descided\n",
            "correction(descided) => described (151); expected decided (149)\n",
            "Processing word: benifit\n",
            "Processing word: concider\n",
            "Processing word: failes\n",
            "correction(failes) => miles (110); expected fails (20)\n",
            "Processing word: carrer\n",
            "correction(carrer) => care (106); expected career (39)\n",
            "Processing word: occurence\n",
            "Processing word: occurence\n",
            "Processing word: cirtain\n",
            "Processing word: poame\n",
            "correction(poame) => home (294); expected poem (6)\n",
            "Processing word: liew\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "Processing word: astablishing\n",
            "Processing word: establising\n",
            "Processing word: diffrent\n",
            "Processing word: lones\n",
            "correction(lones) => one (3371); expected loans (13)\n",
            "Processing word: extreamly\n",
            "Processing word: addresable\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "Processing word: galery\n",
            "Processing word: gallary\n",
            "Processing word: gallerry\n",
            "Processing word: gallrey\n",
            "Processing word: centraly\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "Processing word: familes\n",
            "correction(familes) => miles (110); expected families (45)\n",
            "Processing word: bicycal\n",
            "Processing word: bycicle\n",
            "Processing word: bycycle\n",
            "correction(bycycle) => cycle (1); expected bicycle (1)\n",
            "Processing word: choise\n",
            "correction(choise) => those (1201); expected choice (46)\n",
            "Processing word: opisite\n",
            "Processing word: oppasite\n",
            "Processing word: oppesite\n",
            "Processing word: oppisit\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "Processing word: oppisite\n",
            "Processing word: opposit\n",
            "correction(opposit) => deposit (24); expected opposite (80)\n",
            "Processing word: oppossite\n",
            "Processing word: oppossitte\n",
            "Processing word: cartains\n",
            "correction(cartains) => certain (361); expected curtains (5)\n",
            "Processing word: certans\n",
            "correction(certans) => certain (361); expected curtains (5)\n",
            "Processing word: courtens\n",
            "correction(courtens) => courses (9); expected curtains (5)\n",
            "Processing word: cuaritains\n",
            "Processing word: curtans\n",
            "Processing word: curtians\n",
            "Processing word: curtions\n",
            "correction(curtions) => portions (56); expected curtains (5)\n",
            "Processing word: adress\n",
            "correction(adress) => access (56); expected address (76)\n",
            "Processing word: adres\n",
            "correction(adres) => are (3630); expected address (76)\n",
            "Processing word: liaision\n",
            "correction(liaision) => division (110); expected liaison (1)\n",
            "Processing word: liason\n",
            "correction(liason) => reason (191); expected liaison (1)\n",
            "Processing word: managment\n",
            "Processing word: inconvienient\n",
            "Processing word: inconvient\n",
            "correction(inconvient) => convient (1); expected inconvenient (4)\n",
            "Processing word: inconvinient\n",
            "Processing word: vairiant\n",
            "Processing word: supercede\n",
            "Processing word: superceed\n",
            "correction(superceed) => supervened (1); expected supersede (1)\n",
            "49% of 270 correct (6% unknown) at 22 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_text_bigram(wrong)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: This\n",
            "Given word is in the vocabulary\n",
            "Processing word: tale\n",
            "Given word is in the vocabulary\n",
            "Processing word: grew\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: tjhe\n",
            "Processing word: telling\n",
            "Given word is in the vocabulary\n",
            "Processing word: until\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: ebcame\n",
            "Processing word: a\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Great\n",
            "Given word is in the vocabulary\n",
            "Processing word: War\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Ring\n",
            "Given word is in the vocabulary\n",
            "Processing word: aznd\n",
            "Processing word: included\n",
            "Given word is in the vocabulary\n",
            "Processing word: many\n",
            "Given word is in the vocabulary\n",
            "Processing word: glimpses\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: yet\n",
            "Given word is in the vocabulary\n",
            "Processing word: more\n",
            "Given word is in the vocabulary\n",
            "Processing word: ancient\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: preceded\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: It\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: beun\n",
            "Processing word: soon\n",
            "Given word is in the vocabulary\n",
            "Processing word: after\n",
            "Given word is in the vocabulary\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: Hobbit\n",
            "Processing word: wts\n",
            "Processing word: written\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: before\n",
            "Given word is in the vocabulary\n",
            "Processing word: its\n",
            "Given word is in the vocabulary\n",
            "Processing word: publication\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: 937\n",
            "Given word is in the vocabulary\n",
            "Processing word: buq\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: did\n",
            "Given word is in the vocabulary\n",
            "Processing word: not\n",
            "Given word is in the vocabulary\n",
            "Processing word: go\n",
            "Given word is in the vocabulary\n",
            "Processing word: on\n",
            "Given word is in the vocabulary\n",
            "Processing word: with\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: sequel\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: wpished\n",
            "Processing word: first\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: complete\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: seet\n",
            "Processing word: ni\n",
            "Given word is in the vocabulary\n",
            "Processing word: oder\n",
            "Given word is in the vocabulary\n",
            "Processing word: tche\n",
            "Processing word: myth\n",
            "Given word is in the vocabulary\n",
            "Processing word: olgoy\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: legends\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Elder\n",
            "Given word is in the vocabulary\n",
            "Processing word: Dayis\n",
            "Processing word: which\n",
            "Given word is in the vocabulary\n",
            "Processing word: had\n",
            "Given word is in the vocabulary\n",
            "Processing word: then\n",
            "Given word is in the vocabulary\n",
            "Processing word: been\n",
            "Given word is in the vocabulary\n",
            "Processing word: taking\n",
            "Given word is in the vocabulary\n",
            "Processing word: shape\n",
            "Given word is in the vocabulary\n",
            "Processing word: fos\n",
            "Processing word: some\n",
            "Given word is in the vocabulary\n",
            "Processing word: yaers\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: desired\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: do\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: my\n",
            "Given word is in the vocabulary\n",
            "Processing word: own\n",
            "Given word is in the vocabulary\n",
            "Processing word: satisfaction\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: had\n",
            "Given word is in the vocabulary\n",
            "Processing word: little\n",
            "Given word is in the vocabulary\n",
            "Processing word: hope\n",
            "Given word is in the vocabulary\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: other\n",
            "Given word is in the vocabulary\n",
            "Processing word: people\n",
            "Given word is in the vocabulary\n",
            "Processing word: would\n",
            "Given word is in the vocabulary\n",
            "Processing word: bwe\n",
            "Processing word: interested\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: work\n",
            "Given word is in the vocabulary\n",
            "Processing word: especially\n",
            "Given word is in the vocabulary\n",
            "Processing word: since\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: xas\n",
            "Processing word: prmarily\n",
            "Processing word: linguistic\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: inspiation\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: begun\n",
            "Given word is in the vocabulary\n",
            "Processing word: vin\n",
            "Processing word: order\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: provide\n",
            "Given word is in the vocabulary\n",
            "Processing word: hte\n",
            "Processing word: necessary\n",
            "Given word is in the vocabulary\n",
            "Processing word: background\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: fzr\n",
            "Processing word: Elvish\n",
            "Processing word: tongues\n",
            "Given word is in the vocabulary\n",
            "Unigram accuracy: 0.1415929203539823\n",
            "Bigram accuracy: 0.1415929203539823\n"
          ]
        }
      ],
      "source": [
        "def add_errors(correct_text, error_rate = 0.2):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', correct_text))\n",
        "    corrupted_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    num_of_errors = int(len(found_words) * error_rate)\n",
        "    error_indices = random.sample(range(len(found_words)), num_of_errors)\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrupted_text.append(correct_text[cur_idx:start])\n",
        "\n",
        "        # Introduce errors only for selected words\n",
        "        if idx in error_indices and len(word) > 1:\n",
        "            error_type = random.choice([\"add\", \"delete\", \"replace\", \"swap\"])\n",
        "            if error_type == \"add\":\n",
        "                corrupted_word = random.choice(add_char(word))\n",
        "            elif error_type == \"delete\":\n",
        "                corrupted_word = random.choice(delete_char(word)) if len(word) > 2 else word\n",
        "            elif error_type == \"replace\":\n",
        "                corrupted_word = random.choice(replace_char(word))\n",
        "            elif error_type == \"swap\":\n",
        "                corrupted_word = random.choice(swap_chars(word))\n",
        "        else:\n",
        "            corrupted_word = word \n",
        "\n",
        "        # Append corrupted word\n",
        "        corrupted_text.append(corrupted_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrupted_text.append(correct_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrupted_text), num_of_errors\n",
        "\n",
        "\n",
        "# text fragment from \"The Lord of The Rings\"\n",
        "test_text = \"This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after The Hobbit was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the myth- ology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people would be interested in this work, especially since it was primarily linguistic in inspiration and was begun in order to provide the necessary background of ‘history’ for Elvish tongues.\"\n",
        "test_text_with_errors, added_error_num = add_errors(test_text)\n",
        "\n",
        "\n",
        "def calculate_word_accuracy(original, corrupted, corrected):\n",
        "    initial_words = re.findall(r'\\b\\w+\\b', original)\n",
        "    words_with_errors = re.findall(r'\\b\\w+\\b', corrupted)\n",
        "    corrected_words = re.findall(r'\\b\\w+\\b', corrected)\n",
        "    correctly_corrected_words_count = 0\n",
        "    possible_corrected_words = 0\n",
        "\n",
        "    for initial_word, word_with_error, corrected_word in zip(initial_words, words_with_errors, corrected_words):\n",
        "        if initial_word in unique_words:\n",
        "            possible_corrected_words+=1\n",
        "            if initial_word == corrected_word and initial_word != word_with_error:\n",
        "                correctly_corrected_words_count+=1\n",
        "    \n",
        "    if possible_corrected_words == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return correctly_corrected_words_count / possible_corrected_words\n",
        "\n",
        "corrected_unigram = correct_text(test_text_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_text_with_errors)\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_bigram))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autumn season brought colorful leaves sa artists prepared for aw annual exhibition showcasing contemporary artwerk.\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: autumn\n",
            "Given word is in the vocabulary\n",
            "Processing word: season\n",
            "Given word is in the vocabulary\n",
            "Processing word: brought\n",
            "Given word is in the vocabulary\n",
            "Processing word: colorful\n",
            "Processing word: leaves\n",
            "Given word is in the vocabulary\n",
            "Processing word: sa\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: prepared\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: aw\n",
            "Processing word: annual\n",
            "Given word is in the vocabulary\n",
            "Processing word: exhibition\n",
            "Given word is in the vocabulary\n",
            "Processing word: showcasing\n",
            "Processing word: contemporary\n",
            "Given word is in the vocabulary\n",
            "Processing word: artwerk\n",
            "Unigram accuracy: 0.0\n",
            "Bigram accuracy: 0.08333333333333333\n"
          ]
        }
      ],
      "source": [
        "test_sentence_1 = \"The autumn season brought colorful leaves as artists prepared for an annual exhibition showcasing contemporary artwork.\"\n",
        "# fix the random seed\n",
        "test_sentence_1_with_errors, added_error_num = add_errors(test_sentence_1)\n",
        "print(test_sentence_1_with_errors)\n",
        "\n",
        "corrected_unigram = correct_text(test_sentence_1_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_sentence_1_with_errors)\n",
        "\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_bigram))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As the sun set, an art exhibition opend in the city's cultural center, showcasing contemporary artwork from renowned \n",
            "and emerging visual artists. Te gallery was filled with vibrant paintings, abstract sculptures, and mltimedia installations thbat efplored \n",
            "themes of identit and transformation. Art enthusiasts and ctllectors engaged in thoughtful discsusions about th impact of modern art on society. \n",
            "Meanwhile, the eveny organizers repared ofr an evening panel featuring welbl-known creative professionals discusfsing the future of digital mvdia \n",
            "in the artcstic landscape.\n",
            "Processing word: As\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: sun\n",
            "Given word is in the vocabulary\n",
            "Processing word: set\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: exhibition\n",
            "Given word is in the vocabulary\n",
            "Processing word: opend\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: city\n",
            "Given word is in the vocabulary\n",
            "Processing word: s\n",
            "Given word is in the vocabulary\n",
            "Processing word: cultural\n",
            "Given word is in the vocabulary\n",
            "Processing word: center\n",
            "Given word is in the vocabulary\n",
            "Processing word: showcasing\n",
            "Processing word: contemporary\n",
            "Given word is in the vocabulary\n",
            "Processing word: artwork\n",
            "Processing word: from\n",
            "Given word is in the vocabulary\n",
            "Processing word: renowned\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: emerging\n",
            "Given word is in the vocabulary\n",
            "Processing word: visual\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: Te\n",
            "Given word is in the vocabulary\n",
            "Processing word: gallery\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: filled\n",
            "Given word is in the vocabulary\n",
            "Processing word: with\n",
            "Given word is in the vocabulary\n",
            "Processing word: vibrant\n",
            "Processing word: paintings\n",
            "Given word is in the vocabulary\n",
            "Processing word: abstract\n",
            "Given word is in the vocabulary\n",
            "Processing word: sculptures\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: mltimedia\n",
            "Processing word: installations\n",
            "Processing word: thbat\n",
            "Processing word: efplored\n",
            "Processing word: themes\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: identit\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: transformation\n",
            "Given word is in the vocabulary\n",
            "Processing word: Art\n",
            "Given word is in the vocabulary\n",
            "Processing word: enthusiasts\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: ctllectors\n",
            "Processing word: engaged\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: thoughtful\n",
            "Given word is in the vocabulary\n",
            "Processing word: discsusions\n",
            "Processing word: about\n",
            "Given word is in the vocabulary\n",
            "Processing word: th\n",
            "Given word is in the vocabulary\n",
            "Processing word: impact\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: modern\n",
            "Given word is in the vocabulary\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: on\n",
            "Given word is in the vocabulary\n",
            "Processing word: society\n",
            "Given word is in the vocabulary\n",
            "Processing word: Meanwhile\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: eveny\n",
            "Processing word: organizers\n",
            "Processing word: repared\n",
            "Processing word: ofr\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: evening\n",
            "Given word is in the vocabulary\n",
            "Processing word: panel\n",
            "Given word is in the vocabulary\n",
            "Processing word: featuring\n",
            "Processing word: welbl\n",
            "Processing word: known\n",
            "Given word is in the vocabulary\n",
            "Processing word: creative\n",
            "Given word is in the vocabulary\n",
            "Processing word: professionals\n",
            "Processing word: discusfsing\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: future\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: digital\n",
            "Given word is in the vocabulary\n",
            "Processing word: mvdia\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: artcstic\n",
            "Processing word: landscape\n",
            "Given word is in the vocabulary\n",
            "0.15942028985507245\n",
            "0.15942028985507245\n"
          ]
        }
      ],
      "source": [
        "test_paragraph = \"\"\"As the sun set, an art exhibition opened in the city's cultural center, showcasing contemporary artwork from renowned \n",
        "and emerging visual artists. The gallery was filled with vibrant paintings, abstract sculptures, and multimedia installations that explored \n",
        "themes of identity and transformation. Art enthusiasts and collectors engaged in thoughtful discussions about the impact of modern art on society. \n",
        "Meanwhile, the event organizers prepared for an evening panel featuring well-known creative professionals discussing the future of digital media \n",
        "in the artistic landscape.\"\"\"\n",
        "\n",
        "test_paragraph_with_errors, added_error_num = add_errors(test_paragraph)\n",
        "print(test_paragraph_with_errors)\n",
        "\n",
        "corrected_paragraph_unigram = correct_text(test_paragraph_with_errors)\n",
        "corrected_paragraph_bigram = correct_text_bigram(test_paragraph_with_errors)\n",
        "\n",
        "unigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_unigram)\n",
        "bigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_bigram)\n",
        "\n",
        "print(unigram_accuracy)\n",
        "print(bigram_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment results\n",
        "1. Bigram model with laplase smoothing performed better than without it.\n",
        "Words test file (without context)  74%  | 41% | 49%\n",
        "Accuracy on the Lord of The Rings  0.106 |0.124| 0.142\n",
        "Accuracy on test sentence 0.25 | 0.25 |  \n",
        "Accuracy on the test paragraph  0.13| 0.13| 0.159\n",
        "\n",
        "2. To increase the context utilization => use beam search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
