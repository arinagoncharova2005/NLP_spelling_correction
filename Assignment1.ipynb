{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import heapq\n",
        "random.seed(26)\n",
        "np.random.seed(26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_corpus(corpus_filename):\n",
        "    with open(corpus_filename) as f:\n",
        "        corpus = f.read()\n",
        "        # find words\n",
        "        lowercased_corpus = corpus.lower()\n",
        "        all_words = re.findall(r'\\w+', lowercased_corpus)\n",
        "        for w in all_words:\n",
        "            if w == 'I':\n",
        "                print('Found')\n",
        "                break\n",
        "        unique_words = set(all_words)\n",
        "    return all_words, unique_words\n",
        "\n",
        "def get_words_frequencies(all_words):\n",
        "    word_freq_dict = {}\n",
        "    for word in all_words:\n",
        "        if word in word_freq_dict:\n",
        "            word_freq_dict[word] += 1\n",
        "        else:\n",
        "            word_freq_dict[word] = 1\n",
        "    return word_freq_dict\n",
        "\n",
        "def get_word_prob(word, all_words, word_freq_dict):\n",
        "    # check that the word exista in the vocabulary\n",
        "    if word in word_freq_dict:\n",
        "        return word_freq_dict[word] / len(all_words)\n",
        "    \n",
        "    return 0\n",
        "\n",
        "def add_char(word):\n",
        "    words_with_char_added = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            words_with_char_added.append(word[:i] + char + word[i:])\n",
        "        words_with_char_added.append(word + char)\n",
        "    return words_with_char_added\n",
        "\n",
        "def delete_char(word):\n",
        "    words_with_char_deleted = []\n",
        "    for i in range(len(word)):\n",
        "        words_with_char_deleted.append(word[:i] + word[i+1:])\n",
        "    return words_with_char_deleted\n",
        "\n",
        "def replace_char(word):\n",
        "    words_with_char_replaced = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            new_word = word[:i] + char + word[i+1:]\n",
        "            words_with_char_replaced.append(new_word)\n",
        "    return words_with_char_replaced\n",
        "\n",
        "def swap_chars(word):\n",
        "    words_with_chars_swapped = []\n",
        "    for i in range(len(word)-1):\n",
        "        new_word = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "        words_with_chars_swapped.append(new_word)\n",
        "    return words_with_chars_swapped\n",
        "\n",
        "def filter_existent_words(words, vocabulary):\n",
        "    return [word for word in words if word in vocabulary]\n",
        "\n",
        "def check_existence(word, vocabulary):\n",
        "    if word in vocabulary:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def generate_candidates_edit_1(word):\n",
        "    candidates = []\n",
        "    words_with_char_added = add_char(word)\n",
        "    #existent_words_with_char_added = filter_existent_words(words_with_char_added, vocabulary)\n",
        "    words_with_char_deleted = delete_char(word)\n",
        "    #existent_words_with_char_deleted = filter_existent_words(words_with_char_deleted, vocabulary)\n",
        "    words_with_char_replaced = replace_char(word)\n",
        "    #existent_words_with_char_replaced = filter_existent_words(words_with_char_replaced, vocabulary)\n",
        "    words_with_chars_swapped = swap_chars(word)\n",
        "\n",
        "    candidates.extend(words_with_char_added)\n",
        "    candidates.extend(words_with_char_deleted)\n",
        "    candidates.extend(words_with_char_replaced)\n",
        "    candidates.extend(words_with_chars_swapped)\n",
        "    unique_candidate_words = set(candidates)\n",
        "    if ('corrected' in unique_candidate_words):\n",
        "        print('Corrected found')\n",
        "    return unique_candidate_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of words in corpus: 1115585\n",
            "Num of unique words in corpus: 32198\n"
          ]
        }
      ],
      "source": [
        "all_words, unique_words = process_corpus('big.txt')\n",
        "print(f\"Num of words in corpus: {len(all_words)}\")\n",
        "print(f\"Num of unique words in corpus: {len(unique_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq = get_words_frequencies(all_words)\n",
        "word_freq['cat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_word_simple(word, vocabulary):\n",
        "    word = word.lower()\n",
        "    if check_existence(word, vocabulary):\n",
        "        return word\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(word)\n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "    if 'corrected' in candidates_edit_2:\n",
        "        print('Corrected found')\n",
        "    unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "\n",
        "    all_candidates = []\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, vocabulary)\n",
        "    unique_candidates_edit_2_existent = filter_existent_words(unique_candidates_edit_2, vocabulary)\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates.append((candidate, 1))\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates.append((candidate, 2))\n",
        "    unique_candidates = set(all_candidates)\n",
        "\n",
        "    # sort unique_candidates by the distance and the probability of the word\n",
        "    sorted_candidates = sorted(unique_candidates, key=lambda x: (x[1], -get_word_prob(x[0], all_words, word_freq)))\n",
        "    if len(sorted_candidates) > 0:\n",
        "        best_candidate = sorted_candidates[0]\n",
        "    else:\n",
        "        best_candidate = (word, 0)\n",
        "\n",
        "    return best_candidate[0]\n",
        "\n",
        "def correct_text(given_text):\n",
        "    found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "    cur_idx = 0\n",
        "    corrected_text = []\n",
        "    for cur_word_with_boundaries in found_words:\n",
        "        word = cur_word_with_boundaries.group()\n",
        "        start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "        corrected_word = correct_word_simple(word, unique_words)\n",
        "        # to save the spaces and punctuation\n",
        "        corrected_text.append(given_text[cur_idx:start_idx])\n",
        "        # if the word's characters are all UPPER\n",
        "        if word.isupper():\n",
        "            corrected_word = corrected_word.upper()\n",
        "        # if the first letter is in upper case\n",
        "        elif word.istitle():\n",
        "            corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "        corrected_text.append(corrected_word)\n",
        "        cur_idx = end_idx\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "    corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "    return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am a cat\n"
          ]
        }
      ],
      "source": [
        "corrected_text = correct_text(\"I am a cat7\")\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_word_simple('speling', unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is spelling correction task.\n"
          ]
        }
      ],
      "source": [
        "text = 'It is speling correction task.'\n",
        "corrected_text = correct_text(text)\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king sport'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_example = 'dking sport'\n",
        "correct_text(text_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_1_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word, freq = line.strip().split()\n",
        "            word = word.lower()\n",
        "            word_freq[word] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "def process_2_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word1, word2, freq = line.strip().split()\n",
        "            word1 = word1.lower()\n",
        "            word2 = word2.lower()\n",
        "            bigram = word1 + ' ' + word2\n",
        "            word_freq[bigram] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "# without Laplase smoothing (I commented it)\n",
        "def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq):\n",
        "    lowered_prev_word = prev_word.lower()\n",
        "    lowered_cur_word = cur_word.lower()\n",
        "    bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "    total_single_word_freq = sum(single_word_freq.values())\n",
        "    if bigram in bigram_freq:\n",
        "        if lowered_prev_word in single_word_freq:\n",
        "            return bigram_freq[bigram] / single_word_freq[lowered_prev_word]\n",
        "        else:\n",
        "            return bigram_freq[bigram] / total_single_word_freq\n",
        "    else:\n",
        "        if lowered_cur_word in single_word_freq:\n",
        "            return single_word_freq[lowered_cur_word] / total_single_word_freq\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "# adding Laplase smoothing\n",
        "# def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq):\n",
        "#     lowered_prev_word = prev_word.lower()\n",
        "#     lowered_cur_word = cur_word.lower()\n",
        "#     bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "#     bigram_count = bigram_freq.get(bigram, 0)\n",
        "#     prev_word_count = single_word_freq.get(lowered_prev_word, 0)\n",
        "#     smoothed_prob = (bigram_count + 1)/ (prev_word_count + len(unique_words))\n",
        "#     return smoothed_prob\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "def calculate_word_sequence_prob(words, bigram_freq, single_word_freq, prev_token = '<S>', edit_distance=1):\n",
        "    result = 0\n",
        "    for i in range(len(words)):\n",
        "        if i==0:\n",
        "            prob = calculate_bigram_prob(prev_token, words[i], bigram_freq, single_word_freq)\n",
        "        else:\n",
        "            prob = calculate_bigram_prob(words[i-1], words[i], bigram_freq, single_word_freq)\n",
        "        if prob == 0:\n",
        "            prob = 1e-10\n",
        "        result+= np.log(prob)\n",
        "        \n",
        "        # penalize for number of corrections\n",
        "        result = result - 0.05*edit_distance\n",
        "        # print(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_word_freq = process_1_word_freq('count_1w.txt')\n",
        "bigram_freq = process_2_word_freq('count_2w.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.259822215839525e-05\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('the', 'cat', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5741336593573144e-09\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('t', 'cot', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.6398877749638754\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['the', 'cat'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-12.593434927385083\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['t', 'cot'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def correct_word_bigram(given_word, given_text, given_word_idx):\n",
        "#     print(given_text)\n",
        "#     given_word = given_word.lower()\n",
        "#     if check_existence(given_word, unique_words):\n",
        "#         print('Given word is in the vocabulary')\n",
        "#         return given_word\n",
        "#     all_candidates_with_edit_dist = []\n",
        "#     unique_candidates_edit_1 = generate_candidates_edit_1(given_word)\n",
        "#     print('unique_candidates_edit_1', unique_candidates_edit_1)\n",
        "    \n",
        "#     candidates_edit_2 = []\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "#         candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "#     unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "#     print('unique_candidates_edit_2', len(unique_candidates_edit_2))\n",
        "\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 1))\n",
        "#     for candidate in unique_candidates_edit_2:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 2))\n",
        "\n",
        "#     all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "#     all_unique_candidates_with_edit_dist_existent = filter_existent_words(all_unique_candidates_with_edit_dist, unique_words)\n",
        "#     print('all_unique_candidates_with_edit_dist_existent', all_unique_candidates_with_edit_dist_existent)\n",
        "    \n",
        "#     # find best candidate\n",
        "#     new_probabilities = []\n",
        "#     for (candidate, edit_dist) in all_unique_candidates_with_edit_dist_existent:\n",
        "#         new_word_sequence = list(given_text.copy())\n",
        "#         new_word_sequence[given_word_idx] = candidate\n",
        "#         print('new_word_sequence', new_word_sequence)\n",
        "#         prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "#         new_probabilities.append(prob)\n",
        "#     if len(all_unique_candidates_with_edit_dist_existent) > 0:\n",
        "#         best_candidate = list(all_unique_candidates_with_edit_dist_existent)[new_probabilities.index(max(new_probabilities))]\n",
        "#     else:\n",
        "#         best_candidate = (given_word, 0)\n",
        "#     return best_candidate[0][0]\n",
        "\n",
        "# def correct_text_bigram(given_text):\n",
        "#     found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "#     cur_idx = 0\n",
        "#     corrected_text = []\n",
        "#     cur_word_idx = 0\n",
        "#     for cur_word_with_boundaries in found_words:\n",
        "#         cur_word_idx += 1\n",
        "#         word = cur_word_with_boundaries.group()\n",
        "#         start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "#         corrected_word = correct_word_bigram(word, given_text, cur_idx)\n",
        "#         # to save the spaces and punctuation\n",
        "#         corrected_text.append(given_text[cur_idx:start_idx])\n",
        "#         # if the word's characters are all UPPER\n",
        "#         if word.isupper():\n",
        "#             corrected_word = corrected_word.upper()\n",
        "#         # if the first letter is in upper case\n",
        "#         elif word.istitle():\n",
        "#             corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "#         corrected_text.append(corrected_word)\n",
        "#         cur_idx = end_idx\n",
        "#     corrected_text.append(given_text[cur_idx:])\n",
        "#     corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "#     return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def correct_word_bigram(given_word, given_text_tokens, given_word_idx):\n",
        "    print(\"Processing word:\", given_word)\n",
        "    given_word_lower = given_word.lower()\n",
        "    \n",
        "    # If the word is already correct, return it\n",
        "    if check_existence(given_word_lower, unique_words):\n",
        "        print('Given word is in the vocabulary')\n",
        "        return given_word\n",
        "\n",
        "    # Generate candidate corrections\n",
        "    all_candidates_with_edit_dist = []\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(given_word_lower)\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, unique_words)\n",
        "\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 1))\n",
        "    \n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_candidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_candidates_edit_2)\n",
        "    \n",
        "    unique_candidates_edit_2_existent = filter_existent_words(set(candidates_edit_2), unique_words)\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 2))\n",
        "    \n",
        "    all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "\n",
        "    if not all_unique_candidates_with_edit_dist:\n",
        "        return given_word \n",
        "    # Find the best correction based on probability\n",
        "    new_probabilities = []\n",
        "    for (candidate, edit_dist) in all_unique_candidates_with_edit_dist:\n",
        "        new_word_sequence = given_text_tokens.copy()\n",
        "        new_word_sequence[given_word_idx] = candidate\n",
        "        prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "        new_probabilities.append(prob)\n",
        "    best_candidate = list(all_unique_candidates_with_edit_dist)[new_probabilities.index(max(new_probabilities))][0]\n",
        "\n",
        "\n",
        "    # Preserve capitalization\n",
        "    if given_word.isupper():\n",
        "        return best_candidate.upper()\n",
        "    elif given_word.istitle():\n",
        "        return best_candidate.capitalize()\n",
        "    else:\n",
        "        return best_candidate\n",
        "\n",
        "def correct_text_bigram(given_text):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', given_text))\n",
        "    corrected_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrected_text.append(given_text[cur_idx:start])\n",
        "\n",
        "        corrected_word = correct_word_bigram(word, [m.group() for m in found_words], idx)\n",
        "        \n",
        "        # Append corrected word\n",
        "        corrected_text.append(corrected_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrected_text)\n",
        "\n",
        "def get_possible_corrections(word, vocabulary):\n",
        "    given_word_lower = word.lower()\n",
        "    all_candidates_with_edit_dist = []\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(given_word_lower)\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, vocabulary)\n",
        "\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 1))\n",
        "    \n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_candidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_candidates_edit_2)\n",
        "    \n",
        "    unique_candidates_edit_2_existent = filter_existent_words(set(candidates_edit_2), vocabulary)\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 2))\n",
        "    \n",
        "    all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "    return all_unique_candidates_with_edit_dist\n",
        "\n",
        "\n",
        "\n",
        "def correct_text_bigram_beam_search(given_text, num_of_candidates):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', given_text))\n",
        "    # append initial text\n",
        "    text_candidates = [(given_text, [m.group() for m in found_words], 0)]\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        new_possible_texts = []\n",
        "        for text_candidate, tokens_candidate, prob_candidate in text_candidates:\n",
        "            possible_word_corrections = get_possible_corrections(word, unique_words)\n",
        "\n",
        "            for candidate, edit_dist in possible_word_corrections:\n",
        "                possible_tokens = tokens_candidate.copy()\n",
        "                possible_tokens[idx] = candidate\n",
        "                new_prob = calculate_word_sequence_prob(possible_tokens, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "                new_text = text_candidate[:match.start()] + candidate + text_candidate[match.end():]\n",
        "                new_possible_texts.append((new_text, possible_tokens, new_prob))\n",
        "        \n",
        "    corrected_candidates = heapq.nlargest(num_of_candidates, new_possible_texts, key=lambda x: x[2])\n",
        "    return corrected_candidates[0][0]\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: Hello\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: am\n",
            "Given word is in the vocabulary\n",
            "Processing word: a\n",
            "Given word is in the vocabulary\n",
            "Processing word: student7\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello! I am a student.'"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text_bigram('Hello! I am a student7.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'animal! Iorin a car.'"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text_bigram_beam_search('Animals! I am a cat7.', 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: dking\n",
            "Processing word: sport\n",
            "Given word is in the vocabulary\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ing sport'"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text_bigram('dking sport')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: dking\n",
            "Processing word: species\n",
            "Given word is in the vocabulary\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ing species'"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text_bigram('dking species')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates for 'dking': {'dkinyg', 'dving', 'doing', 'uking', 'dkaing', 'dkwing', 'dkidg', 'sking', 'daking', 'dkiny', 'dtking', 'dkinvg', 'dkifg', 'dkinz', 'dkitng', 'dkong', 'dkingg', 'hking', 'dkixng', 'djking', 'xking', 'dkivng', 'dkilng', 'dning', 'dkiung', 'dkinx', 'dling', 'dying', 'dkinc', 'dkiyng', 'dkeing', 'dkixg', 'dkijng', 'dkicg', 'bdking', 'dkinl', 'dkxing', 'dfing', 'dkibng', 'dkieg', 'dkinw', 'gdking', 'dkiqng', 'bking', 'dkipg', 'dkiang', 'dkcng', 'kking', 'mdking', 'dcking', 'dikng', 'diing', 'dkinig', 'fking', 'dkpng', 'dqking', 'wking', 'zking', 'dkina', 'dkind', 'dcing', 'dvking', 'dkinpg', 'edking', 'ldking', 'yking', 'dkinsg', 'dkigng', 'dkine', 'dkning', 'dfking', 'dgking', 'dkinjg', 'dkang', 'dkisng', 'dkinag', 'dkiag', 'dkyng', 'dwing', 'dging', 'dkfng', 'fdking', 'dkindg', 'rdking', 'dkinb', 'cdking', 'dkizg', 'dknng', 'idking', 'dkink', 'dkding', 'dkilg', 'xdking', 'dkting', 'dxking', 'dkqing', 'dkino', 'dkping', 'duking', 'dsing', 'dbing', 'dkinkg', 'dkrng', 'hdking', 'dkineg', 'dkiog', 'dkibg', 'dkinlg', 'dkidng', 'pking', 'dkming', 'dkinmg', 'dkwng', 'dkikg', 'dkinn', 'ddking', 'dkikng', 'dkinug', 'dkeng', 'dkiqg', 'dkiwng', 'odking', 'dkvng', 'jdking', 'nking', 'dbking', 'qking', 'dkhng', 'dming', 'dmking', 'dkzng', 'dkbing', 'dkiing', 'dkung', 'dknig', 'dkoing', 'dkizng', 'dkimg', 'dkjing', 'deking', 'dping', 'dkgng', 'adking', 'wdking', 'dkin', 'zdking', 'dkinog', 'duing', 'dting', 'deing', 'dkkng', 'dkinbg', 'qdking', 'rking', 'dkbng', 'dkinf', 'dkinng', 'dkking', 'dkmng', 'gking', 'drking', 'dkving', 'aking', 'dzing', 'dpking', 'daing', 'dkqng', 'pdking', 'ndking', 'udking', 'dwking', 'dkinwg', 'dkinfg', 'eking', 'vking', 'dqing', 'dxing', 'dking', 'dkinr', 'dkling', 'dkincg', 'dkhing', 'dkxng', 'dring', 'kdking', 'dkying', 'dhing', 'dkinh', 'dkifng', 'dkimng', 'dkint', 'dkieng', 'dkiyg', 'doking', 'dkinrg', 'dkinxg', 'dklng', 'dkijg', 'sdking', 'dkivg', 'dkiig', 'dksng', 'vdking', 'mking', 'diking', 'dkihng', 'dkini', 'dsking', 'dding', 'dkicng', 'ydking', 'dhking', 'dkig', 'dkirg', 'dkdng', 'dkinp', 'kding', 'dzking', 'dkng', 'dlking', 'dyking', 'dkintg', 'dnking', 'dkigg', 'dkcing', 'dkiong', 'dkiug', 'dkins', 'dkjng', 'dksing', 'dkging', 'iking', 'dkring', 'dkinu', 'dktng', 'dkinv', 'dkingm', 'oking', 'dkinq', 'dkihg', 'lking', 'ding', 'dkzing', 'dkfing', 'dkinm', 'dkinhg', 'dkinqg', 'dkign', 'dkipng', 'cking', 'tking', 'djing', 'dkirng', 'dkisg', 'dkuing', 'dkiwg', 'dkitg', 'tdking', 'jking', 'dkinj', 'dkinzg', 'king'}\n"
          ]
        }
      ],
      "source": [
        "word='dking'\n",
        "candidates = generate_candidates_edit_1(word)\n",
        "print(f\"Candidates for '{word}': {candidates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram count: 0\n",
            "Bigram count: 0\n",
            "Single word count: 9123557\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram count:\", bigram_freq.get(\"dying sport\", 0))\n",
        "print(\"Bigram count:\", bigram_freq.get(\"dying species\", 0))\n",
        "print(\"Single word count:\", single_word_freq.get(\"dying\", 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying trigrams\n",
        "Dataset with trigrams\n",
        "https://calmcode.io/datasets/english_3grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difficulties\n",
        "\n",
        "multiplication of probabilities fastly becomes 0 => use sum of logarithms\n",
        "for unseen words the probabilities are similar as for 'dying species' and 'dyong sport'\n",
        "\n",
        "#### Difficulties\n",
        "**Capturing context**\n",
        "\n",
        "- moving from unigrams to bigrams\n",
        "- moving from **bigrams** to **trigrams**\n",
        "\n",
        "With bigrams for phrases of 2 words the context is not captured. In the given example, for the word `dking` we just see the start token `<S>` and do not see the next word: `sport` or `species`. Therefore, I decided to use trigrams.\n",
        "\n",
        "- no trigram \n",
        "### Ideas\n",
        "- backoff\n",
        "- keyboard layout\n",
        "- dataset larger\n",
        "- forward and backward\n",
        "- несколькр слов подряд некорректных - заменять на скорректированное\n",
        "- использовать стеммы?\n",
        "- добавить swap\n",
        "- стемминг\n",
        "\n",
        "\n",
        "Вместо big можно вот этот попробовать https://www.kaggle.com/datasets/ironicninja/coca-dataset?select=COCA_tokens.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing my unigram model with Norwigs\n",
        "\n",
        "1. I noticed the difference in the training corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32198"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115585"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79809"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('quintessential', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07154004401278254"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('the', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Norvig tests\n",
        "def unit_tests():\n",
        "    assert correct_text('speling') == 'spelling'              # insert\n",
        "    assert correct_text('korrectud') == 'corrected'           # replace 2\n",
        "    assert correct_text('bycycle') == 'bicycle'               # replace\n",
        "    assert correct_text('inconvient') == 'inconvenient'       # insert 2\n",
        "    assert correct_text('arrainged') == 'arranged'            # delete\n",
        "    assert correct_text('peotry') =='poetry'                  # transpose\n",
        "    assert correct_text('peotryy') =='poetry'                 # transpose + delete\n",
        "    assert correct_text('word') == 'word'                     # known\n",
        "    assert correct_text('quintessential') == 'quintessential' # unknown\n",
        "    # assert process_corpus('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
        "    # assert len(unique_words) == 32192\n",
        "    # assert len(all_words) == 1115504\n",
        "    # assert all_words.most_common(10) == [\n",
        "    #  ('the', 79808),\n",
        "    #  ('of', 40024),\n",
        "    #  ('and', 38311),\n",
        "    #  ('to', 28765),\n",
        "    #  ('in', 22020),\n",
        "    #  ('a', 21124),\n",
        "    #  ('that', 12512),\n",
        "    #  ('he', 12401),\n",
        "    #  ('was', 11410),\n",
        "    #  ('it', 10681)]\n",
        "    # assert all_words['the'] == 79808\n",
        "    assert get_word_prob('quintessential', all_words, word_freq) == 0\n",
        "    assert 0.07 < get_word_prob('the', all_words, word_freq) < 0.08\n",
        "    return 'unit_tests pass'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For unigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "correction(contende) => contend (3); expected contented (13)\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "correction(guic) => guns (111); expected juice (5)\n",
            "correction(juce) => june (44); expected juice (5)\n",
            "correction(jucie) => julie (71); expected juice (5)\n",
            "correction(juise) => guise (8); expected juice (5)\n",
            "correction(juse) => just (767); expected juice (5)\n",
            "correction(localy) => local (181); expected locally (10)\n",
            "correction(compair) => company (190); expected compare (29)\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "correction(futher) => father (533); expected further (138)\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "correction(receit) => recent (53); expected receipt (13)\n",
            "correction(receite) => receive (95); expected receipt (13)\n",
            "correction(reciet) => recite (4); expected receipt (13)\n",
            "correction(remined) => remained (231); expected remind (9)\n",
            "correction(annt) => anna (294); expected aunt (52)\n",
            "correction(ther) => the (79809); expected there (2972)\n",
            "correction(totaly) => total (35); expected totally (9)\n",
            "correction(vistid) => viscid (3); expected visited (28)\n",
            "correction(ment) => men (1145); expected meant (113)\n",
            "correction(sorces) => forces (176); expected sources (30)\n",
            "correction(desicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "correction(splened) => opened (216); expected splendid (77)\n",
            "correction(acount) => count (748); expected account (177)\n",
            "correction(semetary) => secretary (52); expected cemetery (2)\n",
            "correction(lates) => later (334); expected latest (17)\n",
            "correction(rember) => member (50); expected remember (161)\n",
            "correction(cak) => can (1095); expected cake (6)\n",
            "correction(chosing) => closing (35); expected choosing (20)\n",
            "correction(rote) => rose (243); expected wrote (149)\n",
            "correction(awfall) => wall (189); expected awful (29)\n",
            "correction(lauf) => last (565); expected laugh (70)\n",
            "correction(laught) => caught (90); expected laugh (70)\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "correction(pomes) => comes (91); expected poems (3)\n",
            "correction(perple) => people (891); expected purple (29)\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "correction(muinets) => muskets (22); expected minutes (146)\n",
            "correction(aranging) => arranging (19); expected arrangeing (0)\n",
            "correction(accesing) => acceding (1); expected accessing (0)\n",
            "correction(stomec) => some (1536); expected stomach (42)\n",
            "correction(embaras) => embargo (7); expected embarrass (0)\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "correction(failes) => failed (63); expected fails (20)\n",
            "correction(poame) => some (1536); expected poem (6)\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "correction(lones) => bones (257); expected loans (13)\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "correction(choise) => choose (54); expected choice (46)\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "correction(cartains) => captains (12); expected curtains (5)\n",
            "correction(certans) => certains (1); expected curtains (5)\n",
            "correction(courtens) => countess (497); expected curtains (5)\n",
            "correction(curtions) => portions (56); expected curtains (5)\n",
            "correction(adress) => dress (138); expected address (76)\n",
            "correction(adres) => acres (36); expected address (76)\n",
            "correction(superceed) => superseded (9); expected supersede (1)\n",
            "74% of 270 correct (6% unknown) at 21 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_word_simple(wrong, unique_words)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "Processing word: contenpted\n",
            "correction(contenpted) => contested (4); expected contented (13)\n",
            "Processing word: contende\n",
            "correction(contende) => content (29); expected contented (13)\n",
            "Processing word: contended\n",
            "Given word is in the vocabulary\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "Processing word: contentid\n",
            "correction(contentid) => content (29); expected contented (13)\n",
            "Processing word: begining\n",
            "correction(begining) => refining (1); expected beginning (143)\n",
            "Processing word: problam\n",
            "correction(problam) => program (43); expected problem (71)\n",
            "Processing word: proble\n",
            "correction(proble) => noble (48); expected problem (71)\n",
            "Processing word: promblem\n",
            "Processing word: proplen\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "Processing word: dirven\n",
            "correction(dirven) => dive (1); expected driven (66)\n",
            "Processing word: exstacy\n",
            "Processing word: ecstacy\n",
            "Processing word: guic\n",
            "correction(guic) => gulf (13); expected juice (5)\n",
            "Processing word: juce\n",
            "correction(juce) => suck (3); expected juice (5)\n",
            "Processing word: jucie\n",
            "Processing word: juise\n",
            "correction(juise) => jose (1); expected juice (5)\n",
            "Processing word: juse\n",
            "correction(juse) => jose (1); expected juice (5)\n",
            "Processing word: localy\n",
            "correction(localy) => lonely (13); expected locally (10)\n",
            "Processing word: compair\n",
            "correction(compair) => complain (14); expected compare (29)\n",
            "Processing word: pronounciation\n",
            "Processing word: transportibility\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "Processing word: miniscule\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "Processing word: independant\n",
            "Processing word: independant\n",
            "Processing word: aranged\n",
            "correction(aranged) => ranger (3); expected arranged (75)\n",
            "Processing word: arrainged\n",
            "Processing word: poartry\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "Processing word: poertry\n",
            "correction(poertry) => perry (5); expected poetry (10)\n",
            "Processing word: poetre\n",
            "correction(poetre) => pierre (1964); expected poetry (10)\n",
            "Processing word: poety\n",
            "correction(poety) => booty (9); expected poetry (10)\n",
            "Processing word: powetry\n",
            "correction(powetry) => pottery (1); expected poetry (10)\n",
            "Processing word: leval\n",
            "correction(leval) => devil (72); expected level (53)\n",
            "Processing word: basicaly\n",
            "Processing word: triangulaur\n",
            "Processing word: unexpcted\n",
            "Processing word: unexpeted\n",
            "correction(unexpeted) => unexpended (1); expected unexpected (45)\n",
            "Processing word: unexspected\n",
            "Processing word: stanerdizing\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "Processing word: varable\n",
            "correction(varable) => marble (10); expected variable (16)\n",
            "Processing word: futher\n",
            "correction(futher) => other (1501); expected further (138)\n",
            "Processing word: monitering\n",
            "Processing word: biscits\n",
            "Processing word: biscutes\n",
            "Processing word: biscuts\n",
            "Processing word: bisquits\n",
            "Processing word: buiscits\n",
            "Processing word: buiscuts\n",
            "Processing word: avaible\n",
            "Processing word: seperate\n",
            "correction(seperate) => desperate (32); expected separate (69)\n",
            "Processing word: neccesary\n",
            "Processing word: necesary\n",
            "Processing word: neccesary\n",
            "Processing word: necassary\n",
            "Processing word: necassery\n",
            "Processing word: neccasary\n",
            "Processing word: defenition\n",
            "Processing word: receit\n",
            "correction(receit) => decent (9); expected receipt (13)\n",
            "Processing word: receite\n",
            "correction(receite) => recent (53); expected receipt (13)\n",
            "Processing word: reciet\n",
            "correction(reciet) => rocket (5); expected receipt (13)\n",
            "Processing word: recipt\n",
            "correction(recipt) => resist (33); expected receipt (13)\n",
            "Processing word: remine\n",
            "Processing word: remined\n",
            "correction(remined) => reminder (6); expected remind (9)\n",
            "Processing word: inetials\n",
            "Processing word: inistals\n",
            "correction(inistals) => installs (1); expected initials (7)\n",
            "Processing word: initails\n",
            "correction(initails) => installs (1); expected initials (7)\n",
            "Processing word: initals\n",
            "correction(initals) => installs (1); expected initials (7)\n",
            "Processing word: intials\n",
            "correction(intials) => indians (47); expected initials (7)\n",
            "Processing word: magnificnet\n",
            "Processing word: magificent\n",
            "Processing word: magnifcent\n",
            "Processing word: magnifecent\n",
            "Processing word: magnifiscant\n",
            "Processing word: magnifisent\n",
            "Processing word: magnificant\n",
            "Processing word: annt\n",
            "correction(annt) => and (38312); expected aunt (52)\n",
            "Processing word: anut\n",
            "correction(anut) => and (38312); expected aunt (52)\n",
            "Processing word: arnt\n",
            "correction(arnt) => and (38312); expected aunt (52)\n",
            "Processing word: intial\n",
            "correction(intial) => intra (35); expected initial (18)\n",
            "Processing word: ther\n",
            "correction(ther) => the (79809); expected there (2972)\n",
            "Processing word: experances\n",
            "Processing word: biult\n",
            "correction(biult) => but (5653); expected built (77)\n",
            "Processing word: totaly\n",
            "correction(totaly) => rotary (1); expected totally (9)\n",
            "Processing word: undersand\n",
            "Processing word: undistand\n",
            "Processing word: southen\n",
            "correction(southen) => south (311); expected southern (194)\n",
            "Processing word: definately\n",
            "correction(definately) => delicately (3); expected definitely (35)\n",
            "Processing word: difinately\n",
            "Processing word: fisited\n",
            "correction(fisited) => limited (79); expected visited (28)\n",
            "Processing word: viseted\n",
            "correction(viseted) => vested (21); expected visited (28)\n",
            "Processing word: vistid\n",
            "correction(vistid) => vista (2); expected visited (28)\n",
            "Processing word: vistied\n",
            "correction(vistied) => vested (21); expected visited (28)\n",
            "Processing word: volantry\n",
            "Processing word: ment\n",
            "correction(ment) => kent (5); expected meant (113)\n",
            "Processing word: recieve\n",
            "correction(recieve) => relieve (20); expected receive (95)\n",
            "Processing word: sorces\n",
            "correction(sorces) => voices (149); expected sources (30)\n",
            "Processing word: wether\n",
            "correction(wether) => other (1501); expected whether (357)\n",
            "Processing word: usefull\n",
            "Processing word: litriture\n",
            "Processing word: valubale\n",
            "Processing word: valuble\n",
            "correction(valuble) => value (106); expected valuable (33)\n",
            "Processing word: desicate\n",
            "correction(desicate) => delicate (54); expected desiccate (0)\n",
            "Processing word: dessicate\n",
            "correction(dessicate) => delicate (54); expected desiccate (0)\n",
            "Processing word: dessiccate\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "Processing word: clearical\n",
            "Processing word: spledid\n",
            "Processing word: splended\n",
            "correction(splended) => blended (5); expected splendid (77)\n",
            "Processing word: splened\n",
            "Processing word: splended\n",
            "correction(splended) => blended (5); expected splendid (77)\n",
            "Processing word: beetween\n",
            "Processing word: completly\n",
            "correction(completly) => complete (144); expected completely (94)\n",
            "Processing word: acount\n",
            "correction(acount) => about (1497); expected account (177)\n",
            "Processing word: cemetary\n",
            "Processing word: semetary\n",
            "correction(semetary) => seminary (2); expected cemetery (2)\n",
            "Processing word: speaical\n",
            "Processing word: specail\n",
            "Processing word: specal\n",
            "correction(specal) => spell (9); expected special (157)\n",
            "Processing word: speical\n",
            "correction(speical) => spinal (39); expected special (157)\n",
            "Processing word: lates\n",
            "correction(lates) => wales (1); expected latest (17)\n",
            "Processing word: latets\n",
            "correction(latets) => gates (34); expected latest (17)\n",
            "Processing word: latiest\n",
            "correction(latiest) => easiest (3); expected latest (17)\n",
            "Processing word: latist\n",
            "correction(latist) => baptist (1); expected latest (17)\n",
            "Processing word: perhapse\n",
            "Processing word: rember\n",
            "correction(rember) => premier (2); expected remember (161)\n",
            "Processing word: remeber\n",
            "correction(remeber) => member (50); expected remember (161)\n",
            "Processing word: rememmer\n",
            "Processing word: rermember\n",
            "Processing word: chaper\n",
            "correction(chaper) => cape (3); expected chapter (464)\n",
            "Processing word: chaphter\n",
            "correction(chaphter) => chatter (8); expected chapter (464)\n",
            "Processing word: chaptur\n",
            "correction(chaptur) => chatter (8); expected chapter (464)\n",
            "Processing word: cak\n",
            "correction(cak) => a (21124); expected cake (6)\n",
            "Processing word: vairious\n",
            "correction(vairious) => vicious (18); expected various (155)\n",
            "Processing word: febuary\n",
            "Processing word: pertend\n",
            "Processing word: protend\n",
            "correction(protend) => protest (19); expected pretend (8)\n",
            "Processing word: prtend\n",
            "correction(prtend) => intend (8); expected pretend (8)\n",
            "Processing word: pritend\n",
            "Processing word: chosing\n",
            "correction(chosing) => hoping (24); expected choosing (20)\n",
            "Processing word: rote\n",
            "correction(rote) => more (1997); expected wrote (149)\n",
            "Processing word: wote\n",
            "correction(wote) => more (1997); expected wrote (149)\n",
            "Processing word: particulaur\n",
            "Processing word: awfall\n",
            "correction(awfall) => fall (124); expected awful (29)\n",
            "Processing word: afful\n",
            "correction(afful) => foul (20); expected awful (29)\n",
            "Processing word: arragment\n",
            "Processing word: chalenges\n",
            "correction(chalenges) => changes (163); expected challenges (2)\n",
            "Processing word: chalenges\n",
            "correction(chalenges) => changes (163); expected challenges (2)\n",
            "Processing word: lagh\n",
            "Processing word: lauf\n",
            "Processing word: laught\n",
            "Processing word: lugh\n",
            "correction(lugh) => luck (28); expected laugh (70)\n",
            "Processing word: ofen\n",
            "correction(ofen) => of (40024); expected often (443)\n",
            "Processing word: offen\n",
            "correction(offen) => oven (7); expected often (443)\n",
            "Processing word: offten\n",
            "Processing word: ofton\n",
            "correction(ofton) => onion (2); expected often (443)\n",
            "Processing word: somone\n",
            "correction(somone) => some (1536); expected someone (160)\n",
            "Processing word: personnell\n",
            "Processing word: uneque\n",
            "correction(uneque) => cheque (2); expected unique (14)\n",
            "Processing word: diagrammaticaally\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "Processing word: discription\n",
            "correction(discription) => disruption (4); expected description (27)\n",
            "Processing word: poims\n",
            "correction(poims) => coins (5); expected poems (3)\n",
            "Processing word: pomes\n",
            "correction(pomes) => home (294); expected poems (3)\n",
            "Processing word: perple\n",
            "correction(perple) => temple (22); expected purple (29)\n",
            "Processing word: perpul\n",
            "correction(perpul) => peru (4); expected purple (29)\n",
            "Processing word: poarple\n",
            "correction(poarple) => parole (3); expected purple (29)\n",
            "Processing word: descide\n",
            "correction(descide) => beside (218); expected decide (33)\n",
            "Processing word: articals\n",
            "correction(articals) => arrivals (7); expected articles (86)\n",
            "Processing word: possition\n",
            "Processing word: extented\n",
            "correction(extented) => expected (126); expected extended (75)\n",
            "Processing word: hierachial\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "Processing word: realy\n",
            "correction(realy) => relay (4); expected really (272)\n",
            "Processing word: relley\n",
            "correction(relley) => relay (4); expected really (272)\n",
            "Processing word: relly\n",
            "correction(relly) => relay (4); expected really (272)\n",
            "Processing word: voteing\n",
            "correction(voteing) => noting (10); expected voting (6)\n",
            "Processing word: comittee\n",
            "Processing word: wantid\n",
            "correction(wantid) => wanting (12); expected wanted (213)\n",
            "Processing word: wonted\n",
            "Given word is in the vocabulary\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "Processing word: benifits\n",
            "Processing word: defenitions\n",
            "correction(defenitions) => definition (23); expected definitions (3)\n",
            "Processing word: scisors\n",
            "Processing word: sissors\n",
            "Processing word: levals\n",
            "correction(levals) => evans (1); expected levels (1)\n",
            "Processing word: paralel\n",
            "correction(paralel) => parade (20); expected parallel (17)\n",
            "Processing word: paralell\n",
            "correction(paralell) => parallels (1); expected parallel (17)\n",
            "Processing word: parrallel\n",
            "Processing word: parralell\n",
            "Processing word: parrallell\n",
            "correction(parrallell) => parallels (1); expected parallel (17)\n",
            "Processing word: accomodation\n",
            "Processing word: acommodation\n",
            "Processing word: acomodation\n",
            "Processing word: planed\n",
            "Given word is in the vocabulary\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "Processing word: hierchy\n",
            "Processing word: transfred\n",
            "correction(transfred) => transfer (20); expected transferred (52)\n",
            "Processing word: muinets\n",
            "correction(muinets) => moines (1); expected minutes (146)\n",
            "Processing word: aranging\n",
            "correction(aranging) => branding (1); expected arrangeing (0)\n",
            "Processing word: accesing\n",
            "correction(accesing) => accusing (1); expected accessing (0)\n",
            "Processing word: stomac\n",
            "correction(stomac) => potomac (3); expected stomach (42)\n",
            "Processing word: stomache\n",
            "Processing word: stomec\n",
            "correction(stomec) => stones (13); expected stomach (42)\n",
            "Processing word: stumache\n",
            "Processing word: unfortunatly\n",
            "correction(unfortunatly) => unfortunate (36); expected unfortunately (11)\n",
            "Processing word: conciderable\n",
            "Processing word: acess\n",
            "correction(acess) => races (8); expected access (56)\n",
            "Processing word: singulaur\n",
            "Processing word: scarcly\n",
            "correction(scarcly) => sharply (43); expected scarcely (65)\n",
            "Processing word: scarecly\n",
            "Processing word: scarely\n",
            "correction(scarely) => safely (11); expected scarcely (65)\n",
            "Processing word: scarsely\n",
            "Processing word: questionaire\n",
            "correction(questionaire) => questionable (3); expected questionnaire (1)\n",
            "Processing word: experance\n",
            "Processing word: experiance\n",
            "Processing word: possable\n",
            "Processing word: reafreshment\n",
            "Processing word: refreshmant\n",
            "Processing word: refresment\n",
            "correction(refresment) => represent (16); expected refreshment (4)\n",
            "Processing word: refressmunt\n",
            "Processing word: embaras\n",
            "correction(embaras) => embargo (7); expected embarrass (0)\n",
            "Processing word: embarass\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "Processing word: vistors\n",
            "correction(vistors) => victory (131); expected visitors (69)\n",
            "Processing word: auxillary\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "Processing word: descided\n",
            "correction(descided) => decides (3); expected decided (149)\n",
            "Processing word: benifit\n",
            "Processing word: concider\n",
            "correction(concider) => concise (4); expected consider (98)\n",
            "Processing word: failes\n",
            "correction(failes) => wales (1); expected fails (20)\n",
            "Processing word: carrer\n",
            "correction(carrer) => carter (1); expected career (39)\n",
            "Processing word: occurence\n",
            "Processing word: occurence\n",
            "Processing word: cirtain\n",
            "correction(cirtain) => britain (77); expected certain (361)\n",
            "Processing word: poame\n",
            "correction(poame) => home (294); expected poem (6)\n",
            "Processing word: liew\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "Processing word: astablishing\n",
            "Processing word: establising\n",
            "Processing word: diffrent\n",
            "Processing word: lones\n",
            "correction(lones) => jones (24); expected loans (13)\n",
            "Processing word: extreamly\n",
            "Processing word: addresable\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "Processing word: galery\n",
            "correction(galery) => glory (49); expected gallery (11)\n",
            "Processing word: gallary\n",
            "correction(gallary) => galaxy (1); expected gallery (11)\n",
            "Processing word: gallerry\n",
            "Processing word: gallrey\n",
            "correction(gallrey) => alley (5); expected gallery (11)\n",
            "Processing word: centraly\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "Processing word: familes\n",
            "correction(familes) => smiles (18); expected families (45)\n",
            "Processing word: bicycal\n",
            "Processing word: bycicle\n",
            "Processing word: bycycle\n",
            "correction(bycycle) => recycle (1); expected bicycle (1)\n",
            "Processing word: choise\n",
            "correction(choise) => christ (32); expected choice (46)\n",
            "Processing word: opisite\n",
            "Processing word: oppasite\n",
            "Processing word: oppesite\n",
            "correction(oppesite) => appetite (12); expected opposite (80)\n",
            "Processing word: oppisit\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "Processing word: oppisite\n",
            "Processing word: opposit\n",
            "correction(opposit) => oppose (14); expected opposite (80)\n",
            "Processing word: oppossite\n",
            "Processing word: oppossitte\n",
            "Processing word: cartains\n",
            "correction(cartains) => curtain (22); expected curtains (5)\n",
            "Processing word: certans\n",
            "Processing word: courtens\n",
            "correction(courtens) => counters (1); expected curtains (5)\n",
            "Processing word: cuaritains\n",
            "Processing word: curtans\n",
            "correction(curtans) => curtis (8); expected curtains (5)\n",
            "Processing word: curtians\n",
            "correction(curtians) => curtis (8); expected curtains (5)\n",
            "Processing word: curtions\n",
            "correction(curtions) => curtis (8); expected curtains (5)\n",
            "Processing word: adress\n",
            "correction(adress) => arrest (58); expected address (76)\n",
            "Processing word: adres\n",
            "correction(adres) => are (3630); expected address (76)\n",
            "Processing word: liaision\n",
            "Processing word: liason\n",
            "correction(liason) => wilson (105); expected liaison (1)\n",
            "Processing word: managment\n",
            "Processing word: inconvienient\n",
            "Processing word: inconvient\n",
            "Processing word: inconvinient\n",
            "Processing word: vairiant\n",
            "Processing word: supercede\n",
            "Processing word: superceed\n",
            "correction(superceed) => superseded (9); expected supersede (1)\n",
            "41% of 270 correct (6% unknown) at 14 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_text_bigram(wrong)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_errors(correct_text, error_rate = 0.2):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', correct_text))\n",
        "    corrupted_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    num_of_errors = int(len(found_words) * error_rate)\n",
        "    error_indices = random.sample(range(len(found_words)), num_of_errors)\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrupted_text.append(correct_text[cur_idx:start])\n",
        "\n",
        "        # Introduce errors only for selected words\n",
        "        if idx in error_indices and len(word) > 1:\n",
        "            error_type = random.choice([\"add\", \"delete\", \"replace\", \"swap\"])\n",
        "            if error_type == \"add\":\n",
        "                corrupted_word = random.choice(add_char(word))\n",
        "            elif error_type == \"delete\":\n",
        "                corrupted_word = random.choice(delete_char(word)) if len(word) > 2 else word\n",
        "            elif error_type == \"replace\":\n",
        "                corrupted_word = random.choice(replace_char(word))\n",
        "            elif error_type == \"swap\":\n",
        "                corrupted_word = random.choice(swap_chars(word))\n",
        "        else:\n",
        "            corrupted_word = word \n",
        "\n",
        "        # Append corrupted word\n",
        "        corrupted_text.append(corrupted_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrupted_text.append(correct_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrupted_text), num_of_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This tle grew in the telling, unti it becwme a history of the Great War of lthe Ring and included many glimpses of the yet more ancient history thft preceded ti. It was bmgun soon after The Hobbit was written and before its publication in 1937; bt I did not go on with this seqel, fro I wished first to complete annd set in order thd myth- ology and legends of the Elder iDays, whijch had hen been making shape for some yeras. I desired to do ths for my own satisfaction, and I had little hope that other speople would be interested in this work, especially since it was primarily linguistic in inspiration nnd was begun in oredr to provide the necessary background of ‘histojry’ fvr Elvish tongues.\n"
          ]
        }
      ],
      "source": [
        "print(test_text_with_errors)\n",
        "error = \"\"\"This tale rgrew in the telling, until it became a history pf the Grea War on the Ring and included many tlimpses of the yet more ancient history tat preceqded it. It was begun soon after Tht Hbbit was written and before its publication in 1937; but I did not go on wih his sequel, for I wished first to compleste and setm in order ithe myth- ology and leegnds of the Elder Das, rhich had then been taking shape for some years. I desired ot do this for my own satisfaction, and I had little hop that other people eould be interested in this work, eepecially since it was primarily linguistic in inspiration and was beguj in order ot provide the necessary background of ‘history’ for Eivish tongues.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test sentence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The hutumn season brought colorful leaves as artists prepared for an sannual exhibition showcasing conetmporary artwork.\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: hutumn\n",
            "Processing word: season\n",
            "Given word is in the vocabulary\n",
            "Processing word: brought\n",
            "Given word is in the vocabulary\n",
            "Processing word: colorful\n",
            "Processing word: leaves\n",
            "Given word is in the vocabulary\n",
            "Processing word: as\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: prepared\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: sannual\n",
            "Processing word: exhibition\n",
            "Given word is in the vocabulary\n",
            "Processing word: showcasing\n",
            "Processing word: conetmporary\n",
            "Processing word: artwork\n",
            "Unigram accuracy: 0.25\n",
            "Bigram accuracy: 0.25\n"
          ]
        }
      ],
      "source": [
        "test_sentence_1 = \"The autumn season brought colorful leaves as artists prepared for an annual exhibition showcasing contemporary artwork.\"\n",
        "test_sentence_1_with_errors = \"The hutumn season brought colorful leaves as artists prepared for an sannual exhibition showcasing conetmporary artwork.\"\n",
        "print(test_sentence_1_with_errors)\n",
        "\n",
        "corrected_unigram = correct_text(test_sentence_1_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_sentence_1_with_errors)\n",
        "\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_bigram))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: As\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: sun\n",
            "Given word is in the vocabulary\n",
            "Processing word: set\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: exhisbition\n",
            "Processing word: opened\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: ity\n",
            "Processing word: s\n",
            "Given word is in the vocabulary\n",
            "Processing word: cultural\n",
            "Given word is in the vocabulary\n",
            "Processing word: cehnter\n",
            "Processing word: showcasing\n",
            "Processing word: contemporary\n",
            "Given word is in the vocabulary\n",
            "Processing word: artwork\n",
            "Processing word: from\n",
            "Given word is in the vocabulary\n",
            "Processing word: renowned\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: emergin\n",
            "Processing word: visual\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: gallery\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: flilled\n",
            "Processing word: with\n",
            "Given word is in the vocabulary\n",
            "Processing word: vibrakt\n",
            "Processing word: paintings\n",
            "Given word is in the vocabulary\n",
            "Processing word: abstract\n",
            "Given word is in the vocabulary\n",
            "Processing word: sculptures\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: multimedia\n",
            "Given word is in the vocabulary\n",
            "Processing word: installations\n",
            "Processing word: taht\n",
            "Processing word: explored\n",
            "Given word is in the vocabulary\n",
            "Processing word: themes\n",
            "Given word is in the vocabulary\n",
            "Processing word: fo\n",
            "Given word is in the vocabulary\n",
            "Processing word: identity\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: transformation\n",
            "Given word is in the vocabulary\n",
            "Processing word: Art\n",
            "Given word is in the vocabulary\n",
            "Processing word: enthusiasts\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: collectors\n",
            "Given word is in the vocabulary\n",
            "Processing word: engaged\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: thoughtful\n",
            "Given word is in the vocabulary\n",
            "Processing word: discussions\n",
            "Given word is in the vocabulary\n",
            "Processing word: about\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: impact\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: modern\n",
            "Given word is in the vocabulary\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: fn\n",
            "Given word is in the vocabulary\n",
            "Processing word: societjy\n",
            "Processing word: Meanwhile\n",
            "Given word is in the vocabulary\n",
            "Processing word: th\n",
            "Given word is in the vocabulary\n",
            "Processing word: event\n",
            "Given word is in the vocabulary\n",
            "Processing word: orgainzers\n",
            "Processing word: prepared\n",
            "Given word is in the vocabulary\n",
            "Processing word: ofr\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: evening\n",
            "Given word is in the vocabulary\n",
            "Processing word: panel\n",
            "Given word is in the vocabulary\n",
            "Processing word: featuring\n",
            "Processing word: well\n",
            "Given word is in the vocabulary\n",
            "Processing word: known\n",
            "Given word is in the vocabulary\n",
            "Processing word: creative\n",
            "Given word is in the vocabulary\n",
            "Processing word: professionals\n",
            "Processing word: discussign\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: future\n",
            "Given word is in the vocabulary\n",
            "Processing word: zof\n",
            "Processing word: digital\n",
            "Given word is in the vocabulary\n",
            "Processing word: media\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: th\n",
            "Given word is in the vocabulary\n",
            "Processing word: artistic\n",
            "Given word is in the vocabulary\n",
            "Processing word: landscape\n",
            "Given word is in the vocabulary\n",
            "0.08695652173913043\n",
            "0.13043478260869565\n"
          ]
        }
      ],
      "source": [
        "test_paragraph = \"\"\"As the sun set, an art exhibition opened in the city's cultural center, showcasing contemporary artwork from renowned \n",
        "and emerging visual artists. The gallery was filled with vibrant paintings, abstract sculptures, and multimedia installations that explored \n",
        "themes of identity and transformation. Art enthusiasts and collectors engaged in thoughtful discussions about the impact of modern art on society. \n",
        "Meanwhile, the event organizers prepared for an evening panel featuring well-known creative professionals discussing the future of digital media \n",
        "in the artistic landscape.\"\"\"\n",
        "\n",
        "# added some mistakes using add_errors function. To reproduce results I fixed the paragraph.\n",
        "test_paragraph_with_errors = \"\"\"As the sun set, an art exhisbition opened in the ity's cultural cehnter, showcasing contemporary artwork from renowned \n",
        "and emergin visual artists. The gallery was flilled with vibrakt paintings, abstract sculptures, and multimedia installations taht explored \n",
        "themes fo identity and transformation. Art enthusiasts and collectors engaged in thoughtful discussions about the impact of modern art fn societjy. \n",
        "Meanwhile, th event orgainzers prepared ofr an evening panel featuring well-known creative professionals discussign the future zof digital media \n",
        "in th artistic landscape.\"\"\"\n",
        "\n",
        "corrected_paragraph_unigram = correct_text(test_paragraph_with_errors)\n",
        "corrected_paragraph_bigram = correct_text_bigram(test_paragraph_with_errors)\n",
        "\n",
        "unigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_unigram)\n",
        "bigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_bigram)\n",
        "\n",
        "print(unigram_accuracy)\n",
        "print(bigram_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the fragment from the book \"The Lord of The Rings\" by John Ronald Reuel Tolkien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: This\n",
            "Given word is in the vocabulary\n",
            "Processing word: tle\n",
            "Processing word: grew\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: telling\n",
            "Given word is in the vocabulary\n",
            "Processing word: unti\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: becwme\n",
            "Processing word: a\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Great\n",
            "Given word is in the vocabulary\n",
            "Processing word: War\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: lthe\n",
            "Processing word: Ring\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: included\n",
            "Given word is in the vocabulary\n",
            "Processing word: many\n",
            "Given word is in the vocabulary\n",
            "Processing word: glimpses\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: yet\n",
            "Given word is in the vocabulary\n",
            "Processing word: more\n",
            "Given word is in the vocabulary\n",
            "Processing word: ancient\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: thft\n",
            "Processing word: preceded\n",
            "Given word is in the vocabulary\n",
            "Processing word: ti\n",
            "Given word is in the vocabulary\n",
            "Processing word: It\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: bmgun\n",
            "Processing word: soon\n",
            "Given word is in the vocabulary\n",
            "Processing word: after\n",
            "Given word is in the vocabulary\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: Hobbit\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: written\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: before\n",
            "Given word is in the vocabulary\n",
            "Processing word: its\n",
            "Given word is in the vocabulary\n",
            "Processing word: publication\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: 1937\n",
            "Processing word: bt\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: did\n",
            "Given word is in the vocabulary\n",
            "Processing word: not\n",
            "Given word is in the vocabulary\n",
            "Processing word: go\n",
            "Given word is in the vocabulary\n",
            "Processing word: on\n",
            "Given word is in the vocabulary\n",
            "Processing word: with\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: seqel\n",
            "Processing word: fro\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: wished\n",
            "Given word is in the vocabulary\n",
            "Processing word: first\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: complete\n",
            "Given word is in the vocabulary\n",
            "Processing word: annd\n",
            "Processing word: set\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: order\n",
            "Given word is in the vocabulary\n",
            "Processing word: thd\n",
            "Processing word: myth\n",
            "Given word is in the vocabulary\n",
            "Processing word: ology\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: legends\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Elder\n",
            "Given word is in the vocabulary\n",
            "Processing word: iDays\n",
            "Processing word: whijch\n",
            "Processing word: had\n",
            "Given word is in the vocabulary\n",
            "Processing word: hen\n",
            "Given word is in the vocabulary\n",
            "Processing word: been\n",
            "Given word is in the vocabulary\n",
            "Processing word: making\n",
            "Given word is in the vocabulary\n",
            "Processing word: shape\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: some\n",
            "Given word is in the vocabulary\n",
            "Processing word: yeras\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: desired\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: do\n",
            "Given word is in the vocabulary\n",
            "Processing word: ths\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: my\n",
            "Given word is in the vocabulary\n",
            "Processing word: own\n",
            "Given word is in the vocabulary\n",
            "Processing word: satisfaction\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: had\n",
            "Given word is in the vocabulary\n",
            "Processing word: little\n",
            "Given word is in the vocabulary\n",
            "Processing word: hope\n",
            "Given word is in the vocabulary\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: other\n",
            "Given word is in the vocabulary\n",
            "Processing word: speople\n",
            "Processing word: would\n",
            "Given word is in the vocabulary\n",
            "Processing word: be\n",
            "Given word is in the vocabulary\n",
            "Processing word: interested\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: work\n",
            "Given word is in the vocabulary\n",
            "Processing word: especially\n",
            "Given word is in the vocabulary\n",
            "Processing word: since\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: primarily\n",
            "Given word is in the vocabulary\n",
            "Processing word: linguistic\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: inspiration\n",
            "Given word is in the vocabulary\n",
            "Processing word: nnd\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: begun\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: oredr\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: provide\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: necessary\n",
            "Given word is in the vocabulary\n",
            "Processing word: background\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: histojry\n",
            "Processing word: fvr\n",
            "Processing word: Elvish\n",
            "Processing word: tongues\n",
            "Given word is in the vocabulary\n",
            "Unigram accuracy: 0.11504424778761062\n",
            "Bigram accuracy: 0.13274336283185842\n"
          ]
        }
      ],
      "source": [
        "# text fragment from \"The Lord of The Rings\"\n",
        "test_text = \"This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after The Hobbit was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the myth- ology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people would be interested in this work, especially since it was primarily linguistic in inspiration and was begun in order to provide the necessary background of ‘history’ for Elvish tongues.\"\n",
        "# test_text_with_errors, added_error_num = add_errors(test_text)\n",
        "test_text_with_errors = \"\"\"This tle grew in the telling, unti it becwme a history of the Great War of lthe Ring and included many glimpses of the yet more ancient history thft preceded ti. It was bmgun soon after The Hobbit was written and before its publication in 1937; bt I did not go on with this seqel, fro I wished first to complete annd set in order thd myth- ology and legends of the Elder iDays, whijch had hen been making shape for some yeras. I desired to do ths for my own satisfaction, and I had little hope that other speople would be interested in this work, especially since it was primarily linguistic in inspiration nnd was begun in oredr to provide the necessary background of ‘histojry’ fvr Elvish tongues.\"\"\"\n",
        "\n",
        "\n",
        "def calculate_word_accuracy(original, corrupted, corrected):\n",
        "    initial_words = re.findall(r'\\b\\w+\\b', original)\n",
        "    words_with_errors = re.findall(r'\\b\\w+\\b', corrupted)\n",
        "    corrected_words = re.findall(r'\\b\\w+\\b', corrected)\n",
        "    correctly_corrected_words_count = 0\n",
        "    possible_corrected_words = 0\n",
        "\n",
        "    for initial_word, word_with_error, corrected_word in zip(initial_words, words_with_errors, corrected_words):\n",
        "        if initial_word in unique_words:\n",
        "            possible_corrected_words+=1\n",
        "            if initial_word == corrected_word and initial_word != word_with_error:\n",
        "                correctly_corrected_words_count+=1\n",
        "    \n",
        "    if possible_corrected_words == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return correctly_corrected_words_count / possible_corrected_words\n",
        "\n",
        "corrected_unigram = correct_text(test_text_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_text_with_errors)\n",
        "# correct_with_beam_search = correct_text_bigram_beam_search(test_text_with_errors, 3)\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_bigram))\n",
        "# print(\"Beam search accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, correct_with_beam_search))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation results\n",
        "\n",
        "I decided to test the implemented models on 5 test cases.\n",
        "\n",
        "✏️ **without context** (to test the model ability of correcting single words)\n",
        "\n",
        "1. Norvig's unit tests\n",
        "2. Words from file `spell-testset1.txt`\n",
        "\n",
        "📝 **with context** (to test the model ability to capture the context)\n",
        "\n",
        "3. Sentence created from the bigrams listed in `count_2w.txt`.\n",
        "4. Paragraph created from the bigrams listed in `count_2w.txt`.\n",
        "5. Fragment from the book \"The Lord of The Rings\".\n",
        "\n",
        "The errors to the context test cases were added using function `add_errors(`) that randomly addes mistakes to the correct words with the given error frequency.\n",
        "\n",
        "### Results\n",
        "\n",
        "| Test case                  | Unigram | Simple bigram | Bigram with Laplase smoothing |\n",
        "|------------------------------------|---------|---------|---------|\n",
        "| Norvig's unit tests               | passed  | passed  | passed   |\n",
        "| Words test file (without context) | 74%     | 41%     | 49%     |\n",
        "| Accuracy on test sentence         | 0.25    | 0.25    | 0.25     |\n",
        "| Accuracy on the test paragraph    | 0.087    | 0.13    | 0.203   |\n",
        "| Accuracy on \"The Lord of The Rings\" fragment | 0.115   | 0.133   | 0.142   |\n",
        "\n",
        "\n",
        "\n",
        "2. To increase the context utilization => use beam search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
