{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_corpus(corpus_filename):\n",
        "    with open(corpus_filename) as f:\n",
        "        corpus = f.read()\n",
        "        # find words\n",
        "        lowercased_corpus = corpus.lower()\n",
        "        all_words = re.findall(r'\\w+', lowercased_corpus)\n",
        "        for w in all_words:\n",
        "            if w == 'I':\n",
        "                print('Found')\n",
        "                break\n",
        "        unique_words = set(all_words)\n",
        "    return all_words, unique_words\n",
        "\n",
        "def get_words_frequencies(all_words):\n",
        "    word_freq_dict = {}\n",
        "    for word in all_words:\n",
        "        if word in word_freq_dict:\n",
        "            word_freq_dict[word] += 1\n",
        "        else:\n",
        "            word_freq_dict[word] = 1\n",
        "    return word_freq_dict\n",
        "\n",
        "def get_word_prob(word, all_words, word_freq_dict):\n",
        "    # check that the word exista in the vocabulary\n",
        "    if word in word_freq_dict:\n",
        "        return word_freq_dict[word] / len(all_words)\n",
        "    \n",
        "    return 0\n",
        "\n",
        "def add_char(word):\n",
        "    words_with_char_added = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            words_with_char_added.append(word[:i] + char + word[i:])\n",
        "        words_with_char_added.append(word + char)\n",
        "    return words_with_char_added\n",
        "\n",
        "def delete_char(word):\n",
        "    words_with_char_deleted = []\n",
        "    for i in range(len(word)):\n",
        "        words_with_char_deleted.append(word[:i] + word[i+1:])\n",
        "    return words_with_char_deleted\n",
        "\n",
        "def replace_char(word):\n",
        "    words_with_char_replaced = []\n",
        "    possible_chars = 'qwertyuiopasdfghjklzxcvbnm'\n",
        "    for i in range(len(word)):\n",
        "        for char in possible_chars:\n",
        "            new_word = word[:i] + char + word[i+1:]\n",
        "            words_with_char_replaced.append(new_word)\n",
        "    return words_with_char_replaced\n",
        "\n",
        "def swap_chars(word):\n",
        "    words_with_chars_swapped = []\n",
        "    for i in range(len(word)-1):\n",
        "        new_word = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "        words_with_chars_swapped.append(new_word)\n",
        "    return words_with_chars_swapped\n",
        "\n",
        "def filter_existent_words(words, vocabulary):\n",
        "    return [word for word in words if word in vocabulary]\n",
        "\n",
        "def check_existence(word, vocabulary):\n",
        "    if word in vocabulary:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def generate_candidates_edit_1(word):\n",
        "    candidates = []\n",
        "    words_with_char_added = add_char(word)\n",
        "    #existent_words_with_char_added = filter_existent_words(words_with_char_added, vocabulary)\n",
        "    words_with_char_deleted = delete_char(word)\n",
        "    #existent_words_with_char_deleted = filter_existent_words(words_with_char_deleted, vocabulary)\n",
        "    words_with_char_replaced = replace_char(word)\n",
        "    #existent_words_with_char_replaced = filter_existent_words(words_with_char_replaced, vocabulary)\n",
        "    words_with_chars_swapped = swap_chars(word)\n",
        "\n",
        "    candidates.extend(words_with_char_added)\n",
        "    candidates.extend(words_with_char_deleted)\n",
        "    candidates.extend(words_with_char_replaced)\n",
        "    candidates.extend(words_with_chars_swapped)\n",
        "    unique_candidate_words = set(candidates)\n",
        "    if ('corrected' in unique_candidate_words):\n",
        "        print('Corrected found')\n",
        "    return unique_candidate_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of words in corpus: 1115585\n",
            "Num of unique words in corpus: 32198\n"
          ]
        }
      ],
      "source": [
        "all_words, unique_words = process_corpus('big.txt')\n",
        "print(f\"Num of words in corpus: {len(all_words)}\")\n",
        "print(f\"Num of unique words in corpus: {len(unique_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq = get_words_frequencies(all_words)\n",
        "word_freq['cat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_word_simple(word, vocabulary):\n",
        "    word = word.lower()\n",
        "    if check_existence(word, vocabulary):\n",
        "        return word\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(word)\n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "    if 'corrected' in candidates_edit_2:\n",
        "        print('Corrected found')\n",
        "    unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "\n",
        "    all_candidates = []\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, vocabulary)\n",
        "    unique_candidates_edit_2_existent = filter_existent_words(unique_candidates_edit_2, vocabulary)\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates.append((candidate, 1))\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates.append((candidate, 2))\n",
        "    unique_candidates = set(all_candidates)\n",
        "\n",
        "    # sort unique_candidates by the distance and the probability of the word\n",
        "    sorted_candidates = sorted(unique_candidates, key=lambda x: (x[1], -get_word_prob(x[0], all_words, word_freq)))\n",
        "    if len(sorted_candidates) > 0:\n",
        "        best_candidate = sorted_candidates[0]\n",
        "    else:\n",
        "        best_candidate = (word, 0)\n",
        "\n",
        "    return best_candidate[0]\n",
        "\n",
        "def correct_text(given_text):\n",
        "    found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "    cur_idx = 0\n",
        "    corrected_text = []\n",
        "    for cur_word_with_boundaries in found_words:\n",
        "        word = cur_word_with_boundaries.group()\n",
        "        start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "        corrected_word = correct_word_simple(word, unique_words)\n",
        "        # to save the spaces and punctuation\n",
        "        corrected_text.append(given_text[cur_idx:start_idx])\n",
        "        # if the word's characters are all UPPER\n",
        "        if word.isupper():\n",
        "            corrected_word = corrected_word.upper()\n",
        "        # if the first letter is in upper case\n",
        "        elif word.istitle():\n",
        "            corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "        corrected_text.append(corrected_word)\n",
        "        cur_idx = end_idx\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "    corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "    return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am a cat\n"
          ]
        }
      ],
      "source": [
        "corrected_text = correct_text(\"I am a cat7\")\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_word_simple('speling', unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is spelling correction task.\n"
          ]
        }
      ],
      "source": [
        "text = 'It is speling correction task.'\n",
        "corrected_text = correct_text(text)\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king sport'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_example = 'dking sport'\n",
        "correct_text(text_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_1_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word, freq = line.strip().split()\n",
        "            word = word.lower()\n",
        "            word_freq[word] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "def process_2_word_freq(filename):\n",
        "    with open(filename) as f:\n",
        "        word_freq = {}\n",
        "        for line in f:\n",
        "            word1, word2, freq = line.strip().split()\n",
        "            word1 = word1.lower()\n",
        "            word2 = word2.lower()\n",
        "            bigram = word1 + ' ' + word2\n",
        "            word_freq[bigram] = int(freq)\n",
        "    return word_freq\n",
        "\n",
        "def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq):\n",
        "    lowered_prev_word = prev_word.lower()\n",
        "    lowered_cur_word = cur_word.lower()\n",
        "    bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "    total_single_word_freq = sum(single_word_freq.values())\n",
        "    if bigram in bigram_freq:\n",
        "        if lowered_prev_word in single_word_freq:\n",
        "            return bigram_freq[bigram] / single_word_freq[lowered_prev_word]\n",
        "        else:\n",
        "            return bigram_freq[bigram] / total_single_word_freq\n",
        "    else:\n",
        "        if lowered_cur_word in single_word_freq:\n",
        "            return single_word_freq[lowered_cur_word] / total_single_word_freq\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# def calculate_bigram_prob(prev_word, cur_word, bigram_freq, single_word_freq, alpha=1):\n",
        "#     lowered_prev_word = prev_word.lower()\n",
        "#     lowered_cur_word = cur_word.lower()\n",
        "#     bigram = lowered_prev_word + ' ' + lowered_cur_word\n",
        "    \n",
        "#     bigram_count = bigram_freq.get(bigram, 0)\n",
        "#     prev_word_count = single_word_freq.get(lowered_prev_word, 0)\n",
        "#     total_vocab_size = len(single_word_freq)\n",
        "\n",
        "#     bigram_prob = (bigram_count + alpha) / (prev_word_count + alpha*(1+total_vocab_size))\n",
        "#     unigram_prob = calculate_unigram_prob(cur_word, single_word_freq, alpha)\n",
        "    \n",
        "#     # l1 = 0.6\n",
        "#     # l2 = 0.4\n",
        "#     # Apply Laplace smoothing\n",
        "#     return bigram_count/(prev_word_count + 1e-10)\n",
        "\n",
        "def calculate_unigram_prob(word, single_word_freq, alpha=1):\n",
        "    lowered_word = word.lower()\n",
        "    word_count = single_word_freq.get(lowered_word, 0)\n",
        "    total_word_count = sum(single_word_freq.values())\n",
        "    total_vocab_size = len(single_word_freq)\n",
        "    \n",
        "    # Apply Laplace smoothing\n",
        "    return (word_count + alpha) / (total_word_count + alpha*(1+total_vocab_size))\n",
        "        \n",
        "\n",
        "def calculate_word_sequence_prob(words, bigram_freq, single_word_freq, prev_token = '<S>', edit_distance=1):\n",
        "    result = 0\n",
        "    for i in range(len(words)):\n",
        "        if i==0:\n",
        "            prob = calculate_bigram_prob(prev_token, words[i], bigram_freq, single_word_freq)\n",
        "        else:\n",
        "            prob = calculate_bigram_prob(words[i-1], words[i], bigram_freq, single_word_freq)\n",
        "            result+= np.log(prob)\n",
        "        if prob == 0:\n",
        "            prob = 1e-10\n",
        "        \n",
        "        # penalize for number of corrections\n",
        "        result = result - 0.05*edit_distance\n",
        "        # print(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_word_freq = process_1_word_freq('count_1w.txt')\n",
        "bigram_freq = process_2_word_freq('count_2w.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.259827996986489e-05\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('the', 'cat', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.079682723190858e-06\n"
          ]
        }
      ],
      "source": [
        "prob = calculate_bigram_prob('t', 'cot', bigram_freq, single_word_freq)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-9.63056932828932\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['the', 'cat'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-12.790683978235908\n"
          ]
        }
      ],
      "source": [
        "word_seq_prob = calculate_word_sequence_prob(['t', 'cot'], bigram_freq, single_word_freq)\n",
        "print(word_seq_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def correct_word_bigram(given_word, given_text, given_word_idx):\n",
        "#     print(given_text)\n",
        "#     given_word = given_word.lower()\n",
        "#     if check_existence(given_word, unique_words):\n",
        "#         print('Given word is in the vocabulary')\n",
        "#         return given_word\n",
        "#     all_candidates_with_edit_dist = []\n",
        "#     unique_candidates_edit_1 = generate_candidates_edit_1(given_word)\n",
        "#     print('unique_candidates_edit_1', unique_candidates_edit_1)\n",
        "    \n",
        "#     candidates_edit_2 = []\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         new_cadidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "#         candidates_edit_2.extend(new_cadidates_edit_2)\n",
        "#     unique_candidates_edit_2 = set(candidates_edit_2)\n",
        "#     print('unique_candidates_edit_2', len(unique_candidates_edit_2))\n",
        "\n",
        "#     for candidate in unique_candidates_edit_1:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 1))\n",
        "#     for candidate in unique_candidates_edit_2:\n",
        "#         all_candidates_with_edit_dist.append((candidate, 2))\n",
        "\n",
        "#     all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "#     all_unique_candidates_with_edit_dist_existent = filter_existent_words(all_unique_candidates_with_edit_dist, unique_words)\n",
        "#     print('all_unique_candidates_with_edit_dist_existent', all_unique_candidates_with_edit_dist_existent)\n",
        "    \n",
        "#     # find best candidate\n",
        "#     new_probabilities = []\n",
        "#     for (candidate, edit_dist) in all_unique_candidates_with_edit_dist_existent:\n",
        "#         new_word_sequence = list(given_text.copy())\n",
        "#         new_word_sequence[given_word_idx] = candidate\n",
        "#         print('new_word_sequence', new_word_sequence)\n",
        "#         prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "#         new_probabilities.append(prob)\n",
        "#     if len(all_unique_candidates_with_edit_dist_existent) > 0:\n",
        "#         best_candidate = list(all_unique_candidates_with_edit_dist_existent)[new_probabilities.index(max(new_probabilities))]\n",
        "#     else:\n",
        "#         best_candidate = (given_word, 0)\n",
        "#     return best_candidate[0][0]\n",
        "\n",
        "# def correct_text_bigram(given_text):\n",
        "#     found_words = re.finditer(r'\\b\\w+\\b', given_text)\n",
        "#     cur_idx = 0\n",
        "#     corrected_text = []\n",
        "#     cur_word_idx = 0\n",
        "#     for cur_word_with_boundaries in found_words:\n",
        "#         cur_word_idx += 1\n",
        "#         word = cur_word_with_boundaries.group()\n",
        "#         start_idx, end_idx = cur_word_with_boundaries.span()\n",
        "#         corrected_word = correct_word_bigram(word, given_text, cur_idx)\n",
        "#         # to save the spaces and punctuation\n",
        "#         corrected_text.append(given_text[cur_idx:start_idx])\n",
        "#         # if the word's characters are all UPPER\n",
        "#         if word.isupper():\n",
        "#             corrected_word = corrected_word.upper()\n",
        "#         # if the first letter is in upper case\n",
        "#         elif word.istitle():\n",
        "#             corrected_word = corrected_word.capitalize()\n",
        "    \n",
        "#         corrected_text.append(corrected_word)\n",
        "#         cur_idx = end_idx\n",
        "#     corrected_text.append(given_text[cur_idx:])\n",
        "#     corrected_text_result = ''.join(corrected_text)\n",
        "        \n",
        "#     return corrected_text_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def correct_word_bigram(given_word, given_text_tokens, given_word_idx):\n",
        "    print(\"Processing word:\", given_word)\n",
        "    given_word_lower = given_word.lower()\n",
        "    \n",
        "    # If the word is already correct, return it\n",
        "    if check_existence(given_word_lower, unique_words):\n",
        "        print('Given word is in the vocabulary')\n",
        "        return given_word\n",
        "\n",
        "    # Generate candidate corrections\n",
        "    all_candidates_with_edit_dist = []\n",
        "    unique_candidates_edit_1 = generate_candidates_edit_1(given_word_lower)\n",
        "    unique_candidates_edit_1_existent = filter_existent_words(unique_candidates_edit_1, unique_words)\n",
        "\n",
        "    for candidate in unique_candidates_edit_1_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 1))\n",
        "    \n",
        "    candidates_edit_2 = []\n",
        "    for candidate in unique_candidates_edit_1:\n",
        "        new_candidates_edit_2 = generate_candidates_edit_1(candidate)\n",
        "        candidates_edit_2.extend(new_candidates_edit_2)\n",
        "    \n",
        "    unique_candidates_edit_2_existent = filter_existent_words(set(candidates_edit_2), unique_words)\n",
        "    for candidate in unique_candidates_edit_2_existent:\n",
        "        all_candidates_with_edit_dist.append((candidate, 2))\n",
        "    \n",
        "    all_unique_candidates_with_edit_dist = set(all_candidates_with_edit_dist)\n",
        "\n",
        "    if not all_unique_candidates_with_edit_dist:\n",
        "        return given_word \n",
        "    # Find the best correction based on probability\n",
        "    new_probabilities = []\n",
        "    for (candidate, edit_dist) in all_unique_candidates_with_edit_dist:\n",
        "        new_word_sequence = given_text_tokens.copy()\n",
        "        new_word_sequence[given_word_idx] = candidate\n",
        "        prob = calculate_word_sequence_prob(new_word_sequence, bigram_freq, single_word_freq, edit_distance=edit_dist)\n",
        "        new_probabilities.append(prob)\n",
        "    best_candidate = list(all_unique_candidates_with_edit_dist)[new_probabilities.index(max(new_probabilities))][0]\n",
        "\n",
        "\n",
        "    # Preserve capitalization\n",
        "    if given_word.isupper():\n",
        "        return best_candidate.upper()\n",
        "    elif given_word.istitle():\n",
        "        return best_candidate.capitalize()\n",
        "    else:\n",
        "        return best_candidate\n",
        "\n",
        "def correct_text_bigram(given_text):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', given_text))\n",
        "    corrected_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrected_text.append(given_text[cur_idx:start])\n",
        "\n",
        "        corrected_word = correct_word_bigram(word, [m.group() for m in found_words], idx)\n",
        "        \n",
        "        # Append corrected word\n",
        "        corrected_text.append(corrected_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrected_text.append(given_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I am a cat. Hello!'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('I am a cat7. Hello!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king sport'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('dking sport')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'king species'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correct_text('dking species')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates for 'dking': {'dkinyg', 'dving', 'doing', 'uking', 'dkaing', 'dkwing', 'dkidg', 'sking', 'daking', 'dkiny', 'dtking', 'dkinvg', 'dkifg', 'dkinz', 'dkitng', 'dkong', 'dkingg', 'hking', 'dkixng', 'djking', 'xking', 'dkivng', 'dkilng', 'dning', 'dkiung', 'dkinx', 'dling', 'dying', 'dkinc', 'dkiyng', 'dkeing', 'dkixg', 'dkijng', 'dkicg', 'bdking', 'dkinl', 'dkxing', 'dfing', 'dkibng', 'dkieg', 'dkinw', 'gdking', 'dkiqng', 'bking', 'dkipg', 'dkiang', 'dkcng', 'kking', 'mdking', 'dcking', 'dikng', 'diing', 'dkinig', 'fking', 'dkpng', 'dqking', 'wking', 'zking', 'dkina', 'dkind', 'dcing', 'dvking', 'dkinpg', 'edking', 'ldking', 'yking', 'dkinsg', 'dkigng', 'dkine', 'dkning', 'dfking', 'dgking', 'dkinjg', 'dkang', 'dkisng', 'dkinag', 'dkiag', 'dkyng', 'dwing', 'dging', 'dkfng', 'fdking', 'dkindg', 'rdking', 'dkinb', 'cdking', 'dkizg', 'dknng', 'idking', 'dkink', 'dkding', 'dkilg', 'xdking', 'dkting', 'dxking', 'dkqing', 'dkino', 'dkping', 'duking', 'dsing', 'dbing', 'dkinkg', 'dkrng', 'hdking', 'dkineg', 'dkiog', 'dkibg', 'dkinlg', 'dkidng', 'pking', 'dkming', 'dkinmg', 'dkwng', 'dkikg', 'dkinn', 'ddking', 'dkikng', 'dkinug', 'dkeng', 'dkiqg', 'dkiwng', 'odking', 'dkvng', 'jdking', 'nking', 'dbking', 'qking', 'dkhng', 'dming', 'dmking', 'dkzng', 'dkbing', 'dkiing', 'dkung', 'dknig', 'dkoing', 'dkizng', 'dkimg', 'dkjing', 'deking', 'dping', 'dkgng', 'adking', 'wdking', 'dkin', 'zdking', 'dkinog', 'duing', 'dting', 'deing', 'dkkng', 'dkinbg', 'qdking', 'rking', 'dkbng', 'dkinf', 'dkinng', 'dkking', 'dkmng', 'gking', 'drking', 'dkving', 'aking', 'dzing', 'dpking', 'daing', 'dkqng', 'pdking', 'ndking', 'udking', 'dwking', 'dkinwg', 'dkinfg', 'eking', 'vking', 'dqing', 'dxing', 'dking', 'dkinr', 'dkling', 'dkincg', 'dkhing', 'dkxng', 'dring', 'kdking', 'dkying', 'dhing', 'dkinh', 'dkifng', 'dkimng', 'dkint', 'dkieng', 'dkiyg', 'doking', 'dkinrg', 'dkinxg', 'dklng', 'dkijg', 'sdking', 'dkivg', 'dkiig', 'dksng', 'vdking', 'mking', 'diking', 'dkihng', 'dkini', 'dsking', 'dding', 'dkicng', 'ydking', 'dhking', 'dkig', 'dkirg', 'dkdng', 'dkinp', 'kding', 'dzking', 'dkng', 'dlking', 'dyking', 'dkintg', 'dnking', 'dkigg', 'dkcing', 'dkiong', 'dkiug', 'dkins', 'dkjng', 'dksing', 'dkging', 'iking', 'dkring', 'dkinu', 'dktng', 'dkinv', 'dkingm', 'oking', 'dkinq', 'dkihg', 'lking', 'ding', 'dkzing', 'dkfing', 'dkinm', 'dkinhg', 'dkinqg', 'dkign', 'dkipng', 'cking', 'tking', 'djing', 'dkirng', 'dkisg', 'dkuing', 'dkiwg', 'dkitg', 'tdking', 'jking', 'dkinj', 'dkinzg', 'king'}\n"
          ]
        }
      ],
      "source": [
        "word='dking'\n",
        "candidates = generate_candidates_edit_1(word)\n",
        "print(f\"Candidates for '{word}': {candidates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram count: 0\n",
            "Bigram count: 0\n",
            "Single word count: 9123557\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram count:\", bigram_freq.get(\"dying sport\", 0))\n",
        "print(\"Bigram count:\", bigram_freq.get(\"dying species\", 0))\n",
        "print(\"Single word count:\", single_word_freq.get(\"dying\", 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying trigrams\n",
        "Dataset with trigrams\n",
        "https://calmcode.io/datasets/english_3grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difficulties\n",
        "\n",
        "multiplication of probabilities fastly becomes 0 => use sum of logarithms\n",
        "for unseen words the probabilities are similar as for 'dying species' and 'dyong sport'\n",
        "\n",
        "#### Difficulties\n",
        "**Capturing context**\n",
        "\n",
        "- moving from unigrams to bigrams\n",
        "- moving from **bigrams** to **trigrams**\n",
        "\n",
        "With bigrams for phrases of 2 words the context is not captured. In the given example, for the word `dking` we just see the start token `<S>` and do not see the next word: `sport` or `species`. Therefore, I decided to use trigrams.\n",
        "\n",
        "- no trigram \n",
        "### Ideas\n",
        "- backoff\n",
        "- keyboard layout\n",
        "- dataset larger\n",
        "- forward and backward\n",
        "- несколькр слов подряд некорректных - заменять на скорректированное\n",
        "- использовать стеммы?\n",
        "- добавить swap\n",
        "- стемминг\n",
        "\n",
        "\n",
        "Вместо big можно вот этот попробовать https://www.kaggle.com/datasets/ironicninja/coca-dataset?select=COCA_tokens.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing my unigram model with Norwigs\n",
        "\n",
        "1. I noticed the difference in the training corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32198"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1115585"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79809"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('quintessential', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07154004401278254"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_word_prob('the', all_words, word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'unit_tests pass'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "# Norvig tests\n",
        "def unit_tests():\n",
        "    assert correct_text('speling') == 'spelling'              # insert\n",
        "    assert correct_text('korrectud') == 'corrected'           # replace 2\n",
        "    assert correct_text('bycycle') == 'bicycle'               # replace\n",
        "    assert correct_text('inconvient') == 'inconvenient'       # insert 2\n",
        "    assert correct_text('arrainged') == 'arranged'            # delete\n",
        "    assert correct_text('peotry') =='poetry'                  # transpose\n",
        "    assert correct_text('peotryy') =='poetry'                 # transpose + delete\n",
        "    assert correct_text('word') == 'word'                     # known\n",
        "    assert correct_text('quintessential') == 'quintessential' # unknown\n",
        "    # assert process_corpus('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
        "    # assert len(unique_words) == 32192\n",
        "    # assert len(all_words) == 1115504\n",
        "    # assert all_words.most_common(10) == [\n",
        "    #  ('the', 79808),\n",
        "    #  ('of', 40024),\n",
        "    #  ('and', 38311),\n",
        "    #  ('to', 28765),\n",
        "    #  ('in', 22020),\n",
        "    #  ('a', 21124),\n",
        "    #  ('that', 12512),\n",
        "    #  ('he', 12401),\n",
        "    #  ('was', 11410),\n",
        "    #  ('it', 10681)]\n",
        "    # assert all_words['the'] == 79808\n",
        "    assert get_word_prob('quintessential', all_words, word_freq) == 0\n",
        "    assert 0.07 < get_word_prob('the', all_words, word_freq) < 0.08\n",
        "    return 'unit_tests pass'\n",
        "\n",
        "unit_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "correction(contende) => contend (3); expected contented (13)\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "correction(guic) => guns (111); expected juice (5)\n",
            "correction(juce) => june (44); expected juice (5)\n",
            "correction(jucie) => julie (71); expected juice (5)\n",
            "correction(juise) => guise (8); expected juice (5)\n",
            "correction(juse) => just (767); expected juice (5)\n",
            "correction(localy) => local (181); expected locally (10)\n",
            "correction(compair) => company (190); expected compare (29)\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "correction(futher) => father (533); expected further (138)\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "correction(receit) => recent (53); expected receipt (13)\n",
            "correction(receite) => receive (95); expected receipt (13)\n",
            "correction(reciet) => recite (4); expected receipt (13)\n",
            "correction(remined) => remained (231); expected remind (9)\n",
            "correction(annt) => anna (294); expected aunt (52)\n",
            "correction(ther) => the (79809); expected there (2972)\n",
            "correction(totaly) => total (35); expected totally (9)\n",
            "correction(vistid) => viscid (3); expected visited (28)\n",
            "correction(ment) => men (1145); expected meant (113)\n",
            "correction(sorces) => forces (176); expected sources (30)\n",
            "correction(desicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "correction(splened) => opened (216); expected splendid (77)\n",
            "correction(acount) => count (748); expected account (177)\n",
            "correction(semetary) => secretary (52); expected cemetery (2)\n",
            "correction(lates) => later (334); expected latest (17)\n",
            "correction(rember) => member (50); expected remember (161)\n",
            "correction(cak) => can (1095); expected cake (6)\n",
            "correction(chosing) => closing (35); expected choosing (20)\n",
            "correction(rote) => rose (243); expected wrote (149)\n",
            "correction(awfall) => wall (189); expected awful (29)\n",
            "correction(lauf) => last (565); expected laugh (70)\n",
            "correction(laught) => caught (90); expected laugh (70)\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "correction(pomes) => comes (91); expected poems (3)\n",
            "correction(perple) => people (891); expected purple (29)\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "correction(muinets) => muskets (22); expected minutes (146)\n",
            "correction(aranging) => arranging (19); expected arrangeing (0)\n",
            "correction(accesing) => acceding (1); expected accessing (0)\n",
            "correction(stomec) => some (1536); expected stomach (42)\n",
            "correction(embaras) => embargo (7); expected embarrass (0)\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "correction(failes) => failed (63); expected fails (20)\n",
            "correction(poame) => some (1536); expected poem (6)\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "correction(lones) => bones (257); expected loans (13)\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "correction(choise) => choose (54); expected choice (46)\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "correction(cartains) => captains (12); expected curtains (5)\n",
            "correction(certans) => certains (1); expected curtains (5)\n",
            "correction(courtens) => countess (497); expected curtains (5)\n",
            "correction(curtions) => portions (56); expected curtains (5)\n",
            "correction(adress) => dress (138); expected address (76)\n",
            "correction(adres) => acres (36); expected address (76)\n",
            "correction(superceed) => superseded (9); expected supersede (1)\n",
            "74% of 270 correct (6% unknown) at 20 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_word_simple(wrong, unique_words)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected found\n",
            "Corrected found\n",
            "Corrected found\n",
            "unit_tests pass\n",
            "Processing word: contenpted\n",
            "Processing word: contende\n",
            "correction(contende) => contente (1); expected contented (13)\n",
            "Processing word: contended\n",
            "Given word is in the vocabulary\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "Processing word: contentid\n",
            "Processing word: begining\n",
            "Processing word: problam\n",
            "Processing word: proble\n",
            "correction(proble) => preble (2); expected problem (71)\n",
            "Processing word: promblem\n",
            "Processing word: proplen\n",
            "correction(proplen) => people (891); expected problem (71)\n",
            "Processing word: dirven\n",
            "Processing word: exstacy\n",
            "correction(exstacy) => eustace (1); expected ecstasy (8)\n",
            "Processing word: ecstacy\n",
            "Processing word: guic\n",
            "correction(guic) => guns (111); expected juice (5)\n",
            "Processing word: juce\n",
            "correction(juce) => june (44); expected juice (5)\n",
            "Processing word: jucie\n",
            "correction(jucie) => julie (71); expected juice (5)\n",
            "Processing word: juise\n",
            "correction(juise) => guise (8); expected juice (5)\n",
            "Processing word: juse\n",
            "correction(juse) => just (767); expected juice (5)\n",
            "Processing word: localy\n",
            "Processing word: compair\n",
            "correction(compair) => complain (14); expected compare (29)\n",
            "Processing word: pronounciation\n",
            "Processing word: transportibility\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "Processing word: miniscule\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "Processing word: independant\n",
            "Processing word: independant\n",
            "Processing word: aranged\n",
            "Processing word: arrainged\n",
            "Processing word: poartry\n",
            "correction(poartry) => portly (6); expected poetry (10)\n",
            "Processing word: poertry\n",
            "Processing word: poetre\n",
            "Processing word: poety\n",
            "correction(poety) => piety (3); expected poetry (10)\n",
            "Processing word: powetry\n",
            "Processing word: leval\n",
            "correction(leval) => legal (52); expected level (53)\n",
            "Processing word: basicaly\n",
            "Processing word: triangulaur\n",
            "Processing word: unexpcted\n",
            "Processing word: unexpeted\n",
            "Processing word: unexspected\n",
            "Processing word: stanerdizing\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "Processing word: varable\n",
            "Processing word: futher\n",
            "Processing word: monitering\n",
            "Processing word: biscits\n",
            "Processing word: biscutes\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "Processing word: biscuts\n",
            "Processing word: bisquits\n",
            "Processing word: buiscits\n",
            "Processing word: buiscuts\n",
            "Processing word: avaible\n",
            "correction(avaible) => avail (19); expected available (37)\n",
            "Processing word: seperate\n",
            "Processing word: neccesary\n",
            "Processing word: necesary\n",
            "Processing word: neccesary\n",
            "Processing word: necassary\n",
            "Processing word: necassery\n",
            "Processing word: neccasary\n",
            "Processing word: defenition\n",
            "Processing word: receit\n",
            "correction(receit) => deceit (4); expected receipt (13)\n",
            "Processing word: receite\n",
            "correction(receite) => receive (95); expected receipt (13)\n",
            "Processing word: reciet\n",
            "correction(reciet) => recite (4); expected receipt (13)\n",
            "Processing word: recipt\n",
            "Processing word: remine\n",
            "Processing word: remined\n",
            "correction(remined) => reined (9); expected remind (9)\n",
            "Processing word: inetials\n",
            "Processing word: inistals\n",
            "Processing word: initails\n",
            "Processing word: initals\n",
            "Processing word: intials\n",
            "Processing word: magnificnet\n",
            "Processing word: magificent\n",
            "Processing word: magnifcent\n",
            "Processing word: magnifecent\n",
            "Processing word: magnifiscant\n",
            "Processing word: magnifisent\n",
            "Processing word: magnificant\n",
            "Processing word: annt\n",
            "correction(annt) => anna (294); expected aunt (52)\n",
            "Processing word: anut\n",
            "correction(anut) => ant (3); expected aunt (52)\n",
            "Processing word: arnt\n",
            "correction(arnt) => ant (3); expected aunt (52)\n",
            "Processing word: intial\n",
            "Processing word: ther\n",
            "correction(ther) => ether (8); expected there (2972)\n",
            "Processing word: experances\n",
            "Processing word: biult\n",
            "Processing word: totaly\n",
            "Processing word: undersand\n",
            "Processing word: undistand\n",
            "Processing word: southen\n",
            "Processing word: definately\n",
            "Processing word: difinately\n",
            "Processing word: fisited\n",
            "Processing word: viseted\n",
            "Processing word: vistid\n",
            "correction(vistid) => viscid (3); expected visited (28)\n",
            "Processing word: vistied\n",
            "Processing word: volantry\n",
            "Processing word: ment\n",
            "correction(ment) => cent (77); expected meant (113)\n",
            "Processing word: recieve\n",
            "Processing word: sorces\n",
            "correction(sorces) => sores (41); expected sources (30)\n",
            "Processing word: wether\n",
            "correction(wether) => ether (8); expected whether (357)\n",
            "Processing word: usefull\n",
            "Processing word: litriture\n",
            "Processing word: valubale\n",
            "Processing word: valuble\n",
            "Processing word: desicate\n",
            "correction(desicate) => dedicate (1); expected desiccate (0)\n",
            "Processing word: dessicate\n",
            "correction(dessicate) => dedicate (1); expected desiccate (0)\n",
            "Processing word: dessiccate\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "Processing word: clearical\n",
            "Processing word: spledid\n",
            "Processing word: splended\n",
            "Processing word: splened\n",
            "correction(splened) => spelled (1); expected splendid (77)\n",
            "Processing word: splended\n",
            "Processing word: beetween\n",
            "Processing word: completly\n",
            "Processing word: acount\n",
            "correction(acount) => amount (91); expected account (177)\n",
            "Processing word: cemetary\n",
            "Processing word: semetary\n",
            "correction(semetary) => seminary (2); expected cemetery (2)\n",
            "Processing word: speaical\n",
            "Processing word: specail\n",
            "Processing word: specal\n",
            "Processing word: speical\n",
            "Processing word: lates\n",
            "correction(lates) => fates (1); expected latest (17)\n",
            "Processing word: latets\n",
            "Processing word: latiest\n",
            "Processing word: latist\n",
            "Processing word: perhapse\n",
            "Processing word: rember\n",
            "correction(rember) => member (50); expected remember (161)\n",
            "Processing word: remeber\n",
            "Processing word: rememmer\n",
            "Processing word: rermember\n",
            "Processing word: chaper\n",
            "Processing word: chaphter\n",
            "Processing word: chaptur\n",
            "Processing word: cak\n",
            "correction(cak) => car (7); expected cake (6)\n",
            "Processing word: vairious\n",
            "Processing word: febuary\n",
            "Processing word: pertend\n",
            "Processing word: protend\n",
            "Processing word: prtend\n",
            "Processing word: pritend\n",
            "Processing word: chosing\n",
            "correction(chosing) => choking (10); expected choosing (20)\n",
            "Processing word: rote\n",
            "correction(rote) => rot (1); expected wrote (149)\n",
            "Processing word: wote\n",
            "correction(wote) => vote (110); expected wrote (149)\n",
            "Processing word: particulaur\n",
            "Processing word: awfall\n",
            "correction(awfall) => befall (3); expected awful (29)\n",
            "Processing word: afful\n",
            "Processing word: arragment\n",
            "correction(arragment) => fragment (16); expected arrangement (35)\n",
            "Processing word: chalenges\n",
            "Processing word: chalenges\n",
            "Processing word: lagh\n",
            "correction(lagh) => lash (8); expected laugh (70)\n",
            "Processing word: lauf\n",
            "correction(lauf) => laid (186); expected laugh (70)\n",
            "Processing word: laught\n",
            "correction(laught) => caught (90); expected laugh (70)\n",
            "Processing word: lugh\n",
            "Processing word: ofen\n",
            "Processing word: offen\n",
            "correction(offen) => offer (57); expected often (443)\n",
            "Processing word: offten\n",
            "Processing word: ofton\n",
            "Processing word: somone\n",
            "Processing word: personnell\n",
            "Processing word: uneque\n",
            "Processing word: diagrammaticaally\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "Processing word: discription\n",
            "Processing word: poims\n",
            "Processing word: pomes\n",
            "correction(pomes) => comes (91); expected poems (3)\n",
            "Processing word: perple\n",
            "correction(perple) => people (891); expected purple (29)\n",
            "Processing word: perpul\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "Processing word: poarple\n",
            "Processing word: descide\n",
            "Processing word: articals\n",
            "Processing word: possition\n",
            "Processing word: extented\n",
            "Processing word: hierachial\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "Processing word: realy\n",
            "correction(realy) => rely (11); expected really (272)\n",
            "Processing word: relley\n",
            "correction(relley) => welled (2); expected really (272)\n",
            "Processing word: relly\n",
            "correction(relly) => rally (4); expected really (272)\n",
            "Processing word: voteing\n",
            "Processing word: comittee\n",
            "Processing word: wantid\n",
            "Processing word: wonted\n",
            "Given word is in the vocabulary\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "Processing word: benifits\n",
            "Processing word: defenitions\n",
            "Processing word: scisors\n",
            "Processing word: sissors\n",
            "Processing word: levals\n",
            "Processing word: paralel\n",
            "Processing word: paralell\n",
            "Processing word: parrallel\n",
            "Processing word: parralell\n",
            "Processing word: parrallell\n",
            "correction(parrallell) => parallels (1); expected parallel (17)\n",
            "Processing word: accomodation\n",
            "Processing word: acommodation\n",
            "Processing word: acomodation\n",
            "Processing word: planed\n",
            "Given word is in the vocabulary\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "Processing word: hierchy\n",
            "Processing word: transfred\n",
            "Processing word: muinets\n",
            "correction(muinets) => miners (19); expected minutes (146)\n",
            "Processing word: aranging\n",
            "correction(aranging) => arranging (19); expected arrangeing (0)\n",
            "Processing word: accesing\n",
            "correction(accesing) => acceding (1); expected accessing (0)\n",
            "Processing word: stomac\n",
            "correction(stomac) => atomic (1); expected stomach (42)\n",
            "Processing word: stomache\n",
            "Processing word: stomec\n",
            "correction(stomec) => stormed (9); expected stomach (42)\n",
            "Processing word: stumache\n",
            "Processing word: unfortunatly\n",
            "Processing word: conciderable\n",
            "Processing word: acess\n",
            "Processing word: singulaur\n",
            "Processing word: scarcly\n",
            "Processing word: scarecly\n",
            "Processing word: scarely\n",
            "Processing word: scarsely\n",
            "Processing word: questionaire\n",
            "Processing word: experance\n",
            "correction(experance) => temperance (7); expected experience (108)\n",
            "Processing word: experiance\n",
            "Processing word: possable\n",
            "Processing word: reafreshment\n",
            "Processing word: refreshmant\n",
            "Processing word: refresment\n",
            "Processing word: refressmunt\n",
            "Processing word: embaras\n",
            "correction(embaras) => embers (4); expected embarrass (0)\n",
            "Processing word: embarass\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "Processing word: vistors\n",
            "Processing word: auxillary\n",
            "correction(auxillary) => axillary (31); expected auxiliary (0)\n",
            "Processing word: descided\n",
            "Processing word: benifit\n",
            "Processing word: concider\n",
            "Processing word: failes\n",
            "Processing word: carrer\n",
            "correction(carrer) => carer (1); expected career (39)\n",
            "Processing word: occurence\n",
            "Processing word: occurence\n",
            "Processing word: cirtain\n",
            "Processing word: poame\n",
            "correction(poame) => place (673); expected poem (6)\n",
            "Processing word: liew\n",
            "correction(liew) => lied (5); expected lieu (7)\n",
            "Processing word: astablishing\n",
            "Processing word: establising\n",
            "Processing word: diffrent\n",
            "Processing word: lones\n",
            "correction(lones) => lines (133); expected loans (13)\n",
            "Processing word: extreamly\n",
            "Processing word: addresable\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "Processing word: galery\n",
            "Processing word: gallary\n",
            "Processing word: gallerry\n",
            "Processing word: gallrey\n",
            "Processing word: centraly\n",
            "correction(centraly) => central (72); expected centrally (0)\n",
            "Processing word: familes\n",
            "Processing word: bicycal\n",
            "Processing word: bycicle\n",
            "Processing word: bycycle\n",
            "Processing word: choise\n",
            "Processing word: opisite\n",
            "Processing word: oppasite\n",
            "Processing word: oppesite\n",
            "Processing word: oppisit\n",
            "correction(oppisit) => oppisit (0); expected opposite (80)\n",
            "Processing word: oppisite\n",
            "Processing word: opposit\n",
            "correction(opposit) => oppose (14); expected opposite (80)\n",
            "Processing word: oppossite\n",
            "Processing word: oppossitte\n",
            "Processing word: cartains\n",
            "correction(cartains) => certains (1); expected curtains (5)\n",
            "Processing word: certans\n",
            "correction(certans) => certains (1); expected curtains (5)\n",
            "Processing word: courtens\n",
            "correction(courtens) => counters (1); expected curtains (5)\n",
            "Processing word: cuaritains\n",
            "Processing word: curtans\n",
            "Processing word: curtians\n",
            "Processing word: curtions\n",
            "correction(curtions) => auctions (1); expected curtains (5)\n",
            "Processing word: adress\n",
            "correction(adress) => dress (138); expected address (76)\n",
            "Processing word: adres\n",
            "correction(adres) => acres (36); expected address (76)\n",
            "Processing word: liaision\n",
            "Processing word: liason\n",
            "Processing word: managment\n",
            "Processing word: inconvienient\n",
            "Processing word: inconvient\n",
            "correction(inconvient) => convient (1); expected inconvenient (4)\n",
            "Processing word: inconvinient\n",
            "Processing word: vairiant\n",
            "Processing word: supercede\n",
            "Processing word: superceed\n",
            "correction(superceed) => supervened (1); expected supersede (1)\n",
            "69% of 270 correct (6% unknown) at 14 words per second \n"
          ]
        }
      ],
      "source": [
        "def spelltest(tests, verbose=True):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correct_text_bigram(wrong)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in unique_words)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, word_freq.get(w, 0), right, word_freq.get(right, 0)))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: This\n",
            "Given word is in the vocabulary\n",
            "Processing word: tale\n",
            "Given word is in the vocabulary\n",
            "Processing word: grew\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: telling\n",
            "Given word is in the vocabulary\n",
            "Processing word: untxil\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_16651/1993728018.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "  result+= np.log(prob)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: became\n",
            "Given word is in the vocabulary\n",
            "Processing word: a\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: hte\n",
            "Processing word: Great\n",
            "Given word is in the vocabulary\n",
            "Processing word: Wazr\n",
            "Processing word: eof\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: Ring\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: included\n",
            "Given word is in the vocabulary\n",
            "Processing word: many\n",
            "Given word is in the vocabulary\n",
            "Processing word: glimpses\n",
            "Given word is in the vocabulary\n",
            "Processing word: vf\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: yet\n",
            "Given word is in the vocabulary\n",
            "Processing word: omre\n",
            "Processing word: ancient\n",
            "Given word is in the vocabulary\n",
            "Processing word: histry\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: preceded\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: It\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: begeun\n",
            "Processing word: soon\n",
            "Given word is in the vocabulary\n",
            "Processing word: after\n",
            "Given word is in the vocabulary\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: Hobbit\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: writtkn\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: befork\n",
            "Processing word: its\n",
            "Given word is in the vocabulary\n",
            "Processing word: publihation\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: 1937\n",
            "Processing word: but\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: did\n",
            "Given word is in the vocabulary\n",
            "Processing word: not\n",
            "Given word is in the vocabulary\n",
            "Processing word: po\n",
            "Given word is in the vocabulary\n",
            "Processing word: on\n",
            "Given word is in the vocabulary\n",
            "Processing word: wth\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: sequel\n",
            "Given word is in the vocabulary\n",
            "Processing word: fro\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: wished\n",
            "Given word is in the vocabulary\n",
            "Processing word: first\n",
            "Given word is in the vocabulary\n",
            "Processing word: tom\n",
            "Given word is in the vocabulary\n",
            "Processing word: complete\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: set\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: order\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: myxh\n",
            "Processing word: ology\n",
            "Processing word: anyd\n",
            "Processing word: legends\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: vhe\n",
            "Processing word: Elder\n",
            "Given word is in the vocabulary\n",
            "Processing word: Days\n",
            "Given word is in the vocabulary\n",
            "Processing word: which\n",
            "Given word is in the vocabulary\n",
            "Processing word: had\n",
            "Given word is in the vocabulary\n",
            "Processing word: then\n",
            "Given word is in the vocabulary\n",
            "Processing word: been\n",
            "Given word is in the vocabulary\n",
            "Processing word: taking\n",
            "Given word is in the vocabulary\n",
            "Processing word: shape\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: some\n",
            "Given word is in the vocabulary\n",
            "Processing word: years\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: desired\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: do\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: fo\n",
            "Given word is in the vocabulary\n",
            "Processing word: my\n",
            "Given word is in the vocabulary\n",
            "Processing word: own\n",
            "Given word is in the vocabulary\n",
            "Processing word: satisfaction\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: I\n",
            "Given word is in the vocabulary\n",
            "Processing word: whad\n",
            "Processing word: little\n",
            "Given word is in the vocabulary\n",
            "Processing word: hopm\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: other\n",
            "Given word is in the vocabulary\n",
            "Processing word: pejple\n",
            "Processing word: would\n",
            "Given word is in the vocabulary\n",
            "Processing word: be\n",
            "Given word is in the vocabulary\n",
            "Processing word: interested\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: this\n",
            "Given word is in the vocabulary\n",
            "Processing word: work\n",
            "Given word is in the vocabulary\n",
            "Processing word: especially\n",
            "Given word is in the vocabulary\n",
            "Processing word: since\n",
            "Given word is in the vocabulary\n",
            "Processing word: it\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: primarily\n",
            "Given word is in the vocabulary\n",
            "Processing word: linguistic\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: inspiration\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: begun\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: order\n",
            "Given word is in the vocabulary\n",
            "Processing word: to\n",
            "Given word is in the vocabulary\n",
            "Processing word: provide\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: necessary\n",
            "Given word is in the vocabulary\n",
            "Processing word: background\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: history\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: lEvish\n",
            "Processing word: tongues\n",
            "Given word is in the vocabulary\n",
            "Unigram accuracy: 0.1504424778761062\n",
            "Bigram accuracy: 0.035398230088495575\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def add_errors(correct_text, error_rate = 0.2):\n",
        "    found_words = list(re.finditer(r'\\b\\w+\\b', correct_text))\n",
        "    corrupted_text = []\n",
        "    cur_idx = 0\n",
        "\n",
        "    num_of_errors = int(len(found_words) * error_rate)\n",
        "    error_indices = random.sample(range(len(found_words)), num_of_errors)\n",
        "\n",
        "    for idx, match in enumerate(found_words):\n",
        "        word = match.group()\n",
        "        start, end = match.span()\n",
        "\n",
        "        # Append text before the word (punctuation, spaces, etc.)\n",
        "        corrupted_text.append(correct_text[cur_idx:start])\n",
        "\n",
        "        # Introduce errors only for selected words\n",
        "        if idx in error_indices and len(word) > 1:\n",
        "            error_type = random.choice([\"add\", \"delete\", \"replace\", \"swap\"])\n",
        "            if error_type == \"add\":\n",
        "                corrupted_word = random.choice(add_char(word))\n",
        "            elif error_type == \"delete\":\n",
        "                corrupted_word = random.choice(delete_char(word)) if len(word) > 2 else word\n",
        "            elif error_type == \"replace\":\n",
        "                corrupted_word = random.choice(replace_char(word))\n",
        "            elif error_type == \"swap\":\n",
        "                corrupted_word = random.choice(swap_chars(word))\n",
        "        else:\n",
        "            corrupted_word = word \n",
        "\n",
        "        # Append corrupted word\n",
        "        corrupted_text.append(corrupted_word)\n",
        "\n",
        "        # Update index to the end of the current word\n",
        "        cur_idx = end\n",
        "\n",
        "    # Append any remaining text (punctuation, spaces after the last word)\n",
        "    corrupted_text.append(correct_text[cur_idx:])\n",
        "\n",
        "    return \"\".join(corrupted_text), num_of_errors\n",
        "\n",
        "\n",
        "# text fragment from \"The Lord of The Rings\"\n",
        "test_text = \"This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after The Hobbit was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the myth- ology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people would be interested in this work, especially since it was primarily linguistic in inspiration and was begun in order to provide the necessary background of ‘history’ for Elvish tongues.\"\n",
        "test_text_with_errors, added_error_num = add_errors(test_text)\n",
        "\n",
        "\n",
        "def calculate_word_accuracy(original, corrupted, corrected):\n",
        "    initial_words = re.findall(r'\\b\\w+\\b', original)\n",
        "    words_with_errors = re.findall(r'\\b\\w+\\b', corrupted)\n",
        "    corrected_words = re.findall(r'\\b\\w+\\b', corrected)\n",
        "    correctly_corrected_words_count = 0\n",
        "    possible_corrected_words = 0\n",
        "\n",
        "    for initial_word, word_with_error, corrected_word in zip(initial_words, words_with_errors, corrected_words):\n",
        "        if initial_word in unique_words:\n",
        "            possible_corrected_words+=1\n",
        "            if initial_word == corrected_word and initial_word != word_with_error:\n",
        "                correctly_corrected_words_count+=1\n",
        "    \n",
        "    if possible_corrected_words == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return correctly_corrected_words_count / possible_corrected_words\n",
        "\n",
        "corrected_unigram = correct_text(test_text_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_text_with_errors)\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_text, test_text_with_errors, corrected_bigram))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autumn season broujght colorful leaves as artists prepared for an annual exhibitaion showcasing contemporary artwrok.\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: autumn\n",
            "Given word is in the vocabulary\n",
            "Processing word: season\n",
            "Given word is in the vocabulary\n",
            "Processing word: broujght\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_16651/1993728018.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "  result+= np.log(prob)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: colorful\n",
            "Processing word: leaves\n",
            "Given word is in the vocabulary\n",
            "Processing word: as\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: prepared\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: annual\n",
            "Given word is in the vocabulary\n",
            "Processing word: exhibitaion\n",
            "Processing word: showcasing\n",
            "Processing word: contemporary\n",
            "Given word is in the vocabulary\n",
            "Processing word: artwrok\n",
            "Unigram accuracy: 0.16666666666666666\n",
            "Bigram accuracy: 0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "test_sentence_1 = \"The autumn season brought colorful leaves as artists prepared for an annual exhibition showcasing contemporary artwork.\"\n",
        "# fix the random seed\n",
        "test_sentence_1_with_errors, added_error_num = add_errors(test_sentence_1)\n",
        "print(test_sentence_1_with_errors)\n",
        "\n",
        "corrected_unigram = correct_text(test_sentence_1_with_errors)\n",
        "corrected_bigram = correct_text_bigram(test_sentence_1_with_errors)\n",
        "\n",
        "print(\"Unigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_unigram))\n",
        "print(\"Bigram accuracy:\", calculate_word_accuracy(test_sentence_1, test_sentence_1_with_errors, corrected_bigram))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As teh sn se, an art exhibition opened in the city's cultural cener, showcasing contemporary artwork from renowned \n",
            "and emerging visual artists. The gallery was filled eith vibrant piantings, abstract sculptures, and multimedia installatins that explored \n",
            "themes of identity hnd transformation. Art enthusiasts and collectors engaged in thoughtfuv discussions about the impact of modenr art on soiety. \n",
            "Meanwhile, the event organizers prepared for an evening panel featuring well-knowb creative professionals idscussing the ftuure fo digital media \n",
            "in the artistic landscape.\n",
            "Processing word: As\n",
            "Given word is in the vocabulary\n",
            "Processing word: teh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_16651/1993728018.py:71: RuntimeWarning: divide by zero encountered in log\n",
            "  result+= np.log(prob)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing word: sn\n",
            "Given word is in the vocabulary\n",
            "Processing word: se\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: exhibition\n",
            "Given word is in the vocabulary\n",
            "Processing word: opened\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: city\n",
            "Given word is in the vocabulary\n",
            "Processing word: s\n",
            "Given word is in the vocabulary\n",
            "Processing word: cultural\n",
            "Given word is in the vocabulary\n",
            "Processing word: cener\n",
            "Processing word: showcasing\n",
            "Processing word: contemporary\n",
            "Given word is in the vocabulary\n",
            "Processing word: artwork\n",
            "Processing word: from\n",
            "Given word is in the vocabulary\n",
            "Processing word: renowned\n",
            "Given word is in the vocabulary\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: emerging\n",
            "Given word is in the vocabulary\n",
            "Processing word: visual\n",
            "Given word is in the vocabulary\n",
            "Processing word: artists\n",
            "Given word is in the vocabulary\n",
            "Processing word: The\n",
            "Given word is in the vocabulary\n",
            "Processing word: gallery\n",
            "Given word is in the vocabulary\n",
            "Processing word: was\n",
            "Given word is in the vocabulary\n",
            "Processing word: filled\n",
            "Given word is in the vocabulary\n",
            "Processing word: eith\n",
            "Processing word: vibrant\n",
            "Processing word: piantings\n",
            "Processing word: abstract\n",
            "Given word is in the vocabulary\n",
            "Processing word: sculptures\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: multimedia\n",
            "Given word is in the vocabulary\n",
            "Processing word: installatins\n",
            "Processing word: that\n",
            "Given word is in the vocabulary\n",
            "Processing word: explored\n",
            "Given word is in the vocabulary\n",
            "Processing word: themes\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: identity\n",
            "Given word is in the vocabulary\n",
            "Processing word: hnd\n",
            "Processing word: transformation\n",
            "Given word is in the vocabulary\n",
            "Processing word: Art\n",
            "Given word is in the vocabulary\n",
            "Processing word: enthusiasts\n",
            "Processing word: and\n",
            "Given word is in the vocabulary\n",
            "Processing word: collectors\n",
            "Given word is in the vocabulary\n",
            "Processing word: engaged\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: thoughtfuv\n",
            "Processing word: discussions\n",
            "Given word is in the vocabulary\n",
            "Processing word: about\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: impact\n",
            "Given word is in the vocabulary\n",
            "Processing word: of\n",
            "Given word is in the vocabulary\n",
            "Processing word: modenr\n",
            "Processing word: art\n",
            "Given word is in the vocabulary\n",
            "Processing word: on\n",
            "Given word is in the vocabulary\n",
            "Processing word: soiety\n",
            "Processing word: Meanwhile\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: event\n",
            "Given word is in the vocabulary\n",
            "Processing word: organizers\n",
            "Processing word: prepared\n",
            "Given word is in the vocabulary\n",
            "Processing word: for\n",
            "Given word is in the vocabulary\n",
            "Processing word: an\n",
            "Given word is in the vocabulary\n",
            "Processing word: evening\n",
            "Given word is in the vocabulary\n",
            "Processing word: panel\n",
            "Given word is in the vocabulary\n",
            "Processing word: featuring\n",
            "Processing word: well\n",
            "Given word is in the vocabulary\n",
            "Processing word: knowb\n",
            "Processing word: creative\n",
            "Given word is in the vocabulary\n",
            "Processing word: professionals\n",
            "Processing word: idscussing\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: ftuure\n",
            "Processing word: fo\n",
            "Given word is in the vocabulary\n",
            "Processing word: digital\n",
            "Given word is in the vocabulary\n",
            "Processing word: media\n",
            "Given word is in the vocabulary\n",
            "Processing word: in\n",
            "Given word is in the vocabulary\n",
            "Processing word: the\n",
            "Given word is in the vocabulary\n",
            "Processing word: artistic\n",
            "Given word is in the vocabulary\n",
            "Processing word: landscape\n",
            "Given word is in the vocabulary\n",
            "0.14492753623188406\n",
            "0.043478260869565216\n"
          ]
        }
      ],
      "source": [
        "test_paragraph = \"\"\"As the sun set, an art exhibition opened in the city's cultural center, showcasing contemporary artwork from renowned \n",
        "and emerging visual artists. The gallery was filled with vibrant paintings, abstract sculptures, and multimedia installations that explored \n",
        "themes of identity and transformation. Art enthusiasts and collectors engaged in thoughtful discussions about the impact of modern art on society. \n",
        "Meanwhile, the event organizers prepared for an evening panel featuring well-known creative professionals discussing the future of digital media \n",
        "in the artistic landscape.\"\"\"\n",
        "\n",
        "test_paragraph_with_errors, added_error_num = add_errors(test_paragraph)\n",
        "print(test_paragraph_with_errors)\n",
        "\n",
        "corrected_paragraph_unigram = correct_text(test_paragraph_with_errors)\n",
        "corrected_paragraph_bigram = correct_text_bigram(test_paragraph_with_errors)\n",
        "\n",
        "unigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_unigram)\n",
        "bigram_accuracy = calculate_word_accuracy(test_paragraph, test_paragraph_with_errors, corrected_paragraph_bigram)\n",
        "\n",
        "print(unigram_accuracy)\n",
        "print(bigram_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
